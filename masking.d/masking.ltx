\documentclass[draft,12pt]{amsart}

\usepackage[final]{listings}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\usepackage{arydshln}

\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{url}
\usepackage[final]{hyperref}
\usepackage{placeins}
\usepackage{mfirstuc}
\usepackage{ifdraft}
\usepackage[obeyDraft]{todonotes}
\usepackage{datetime2}
\ifdraft{
    \usepackage{draftwatermark}
}{}

% Fix conflict between todonotes and amsart packages
% See http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/todonotes/todonotes.pdf,
% p. 10.
\makeatletter
\providecommand\@dotsep{5}
\def\listtodoname{List of TODOs}
\def\listoftodos{\@starttoc{tdo}\listtodoname}
\makeatother

\makeatletter
\renewcommand{\listofalgorithms}{\@starttoc{loa}{List of algorithms}}
\let\l@algorithm=\l@figure
\makeatother

% Re hbox's, see
% https://tex.stackexchange.com/questions/241343/what-is-the-meaning-of-fussy-sloppy-emergencystretch-tolerance-hbadness
\raggedbottom

% Replaced by todonotes package
% \newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}

\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}
\newcommand{\mymathbin}[1]{\mathbin{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\mult}{\mathbin{\ast}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}\xspace}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

\newcommand{\lastix}[1]{\ensuremath{\##1}}
\newcommand{\Vlastix}[1]{\ensuremath{\lastix{\V{#1}}}}
\newcommand{\VlastElement}[1]{\Velement{#1}{\Vlastix{#1}}}
\newcommand{\element}[2]{\ensuremath{\mathop{#1}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\Velement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\VVelement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[ \V{#2}
    \right]\mathclose{} }}

\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\mcolon}{\mathrel:}
\newcommand{\mcoloncolon}{\mathrel{\vcenter{\hbox{$::$}}}}

\newcommand{\suchthat}{\mcoloncolon}
\newcommand{\quantify}[3]{% quantifier, binding, predicate
    \ensuremath{\mathrel{#1}#2 \; \suchthat \; #3}%
}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\V}[1]{\ensuremath{\texttt{\mbox{#1}}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\Rightarrow}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

% For the type subscript for powersets
\newcommand{\setOf}[1]{{{#1}^{\mbox{\normalsize $\ast$}}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }

% use internal counter of algorithmicx
\newcommand{\algnested}{ %
  \makeatletter\ALG@nested\makeatother %
}

\newcommand{\algstrut}{\rule{0pt}{.85\baselineskip}}

\newcommand{\myparbox}[2][\algparwidth]{%
  \parbox[t]{#1}{%
  % \linespread{1.15}\selectfont
  \ignorespaces#2\par%
  }%
}

\newcommand{\parcomment}[2]{\hspace{\dimexpr \algorithmicindent * #1}$\triangleright$ #2}

% \newcommand{\rawparcomment}[2][\algparwidth]{%
  % \myparbox[#1]{#2}%
% }
% \newcommand{\parcomment}[2][\algparwidth]{%
  % \myparbox[#1]{$\triangleright$ #2}%
% }
% \newcommand{\parcomment}[1]{\hspace{\dimexpr \algorithmicindent - \labelwidth }$\triangleright$
  % \protect\parbox[t]{\dimexpr \textwidth - (\algorithmicindent - \labelwidth)}{#1}%
% }

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newcommand{\mname}[1]{\mbox{\sf #1}}
% [O]perator [A]pplication
\newcommand{\Oname}[1]{\mname{#1}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\OA}[2]{\ensuremath{%
    \mathop{\Oname{#1}}\mathopen{}\left(#2\right)\mathclose{}%
}}
\newcommand{\VOA}[2]{\OA{#1}{\V{#2}}}
% [F]unction [A]pplication, replaces \myfn
\newcommand{\smallrawfn}[2]{\ensuremath{\mathop{#1}(#2)}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\rawfn}[2]{\ensuremath{\mathop{#1}\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\Fname}[1]{\V{#1}}
\newcommand{\FA}[2]{\ensuremath{\rawfn{\mathop{\Fname{#1}}}{#2}}}
\newcommand{\VFA}[2]{\FA{#1}{\V{#2}}}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

% I like to silence underfull hbox messages.
% This is syntactic sugar for doing that.
\newenvironment{MYsloppy}[1][3em]{\par\tolerance9999 \emergencystretch#1\relax}{\par}
% This form allows the resetting of hbadness, which may be necessary to
% shut up an "underfull" warning.
\newenvironment{MYsloppyB}[2]{\par\tolerance9999 \emergencystretch#1 \hbadness#2\relax}{\par}
% This form is for a trick:  Tolerances are evaluated line by line, so a tolerance
% less than 9999 may produce a better looking paragraph and reduce the "badness".
\newenvironment{MYsloppyC}[3]{\par\tolerance#3 \emergencystretch#1 \hbadness#2\relax}{\par}

\title
  [Masking Thoughts]
  {Thoughts on optimizing the computation of masks}

\author{Jeffrey Kegler}
\ifdraft{
    % legacy draftwatermark options -- update?
    \SetWatermarkLightness{0}
    \SetWatermarkText{DRAFT \DTMnow{} DRAFT}
    \SetWatermarkAngle{0}
    % \SetWatermarkScale{1}
    % \SetWatermarkHorCenter{1.5in}
    \SetWatermarkVerCenter{.5in}
}{}
\ifdraft{ \thanks{DRAFT as of \DTMnow} }{}
\date{\DTMnow.}

\begin{document}

\maketitle
\tableofcontents

\section{About this document}
\label{sec:about-me}

This document outlines a method for optimizing mask computation in
Guidance.
It assumes some familiarity with the code in
llguidance\cite{llguidance}.

\section{Conventions}
\begin{itemize}
\item ``Iff'' (or ``iff'') abbreviates
	``if and only if''.
\item Ranges are inclusive start,
	exclusive end so that, for example,
		\begin{gather*}
			\element{(\texttt{"abcd"})}{0,2} = \texttt{"ab"} \text{, and} \\
			\element{(\texttt{"abcd"})}{2,3} = \texttt{"c"}.
		\end{gather*}
\end{itemize}

\section{The Basic Idea}

The basic idea behind the suggestions in this document is more
aggressive use of the Ruby Slippers.
The Ruby Slippers is
a parsing technique discovered independently by Jeffrey Kegler%
\cite{Kegler2011a}\cite{Kegler2011b}\cite{Kegler2023}
and
Michal Moskal\cite{llguidance}.
It takes advantage of the ability of certain variants of the Earley
parser to tell the application
exactly which terminals will be acceptable at
any point in the parse.
This allows the application to change the lexeme stream to match
the parser's expectations.
The application could, for example, invent a lexeme on the fly
to accomodate the parser.\footnote{%
    The name ``Ruby Slippers'' comes from a incident in the movie
    {\it Wizard of Oz}.  In it Dorothy, the protagonist, is trying
    to return to her home in Kansas.  After many adventures she
    discovers that a pair of ruby slippers, which she has been
    wearing throughout the movie, grant her this ability.
    She has merely to click her heels and wish.}

The Ruby Slippers are already in use
in llguidance.
An \url{allowed\_lexemes} vector is computed,
and applied at certain points.

Looking ahead
in this document, we are going to claim
that the Ruby Slippers can be used earlier and more extensively,
that much of the work of applying the
Ruby Slippers can be done when the grammar is compiled, rather
than ``online'', as the parse progresses,
and that other work, not efficient to do at pre-computation,
can be done ``lazily'' and cached.\footnote{%
        For discussion of the trade-offs between precomputation and
        lazy computation with caching, see Section \ref{sec:caching}.
        }
It is hoped that this will allow considerable savings in time.

\FloatBarrier

\section{The structure of the byte stream}
\label{sec:byte-stream}

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
	\multicolumn{2}{c}{TB} &
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\rule{30pt}{0pt} &
	\multicolumn{2}{c|}{ALA} &
\multicolumn{2}{p{72pt}}{\centering Current} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
	\multicolumn{2}{p{72pt}|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{p{30pt}|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{p{30pt}|}{\rule{30pt}{0pt}} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-5}
	\multicolumn{2}{|p{72pt}|}{\centering \hyphenpenalty=10000 accepted lexeme} &
	% 30pt * 5 + 12pt * 4 = 198pt
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt} working lexeme}
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-6}
\rule{30pt}{0pt} &
	\multicolumn{2}{|p{72pt}|}{\centering committed token} &
	% 30pt * 3 + 12pt * 2 = 114pt
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt} working token} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-7}
	\multicolumn{4}{|p{156pt}|}{\centering seen input} &
	\multicolumn{3}{|p{114pt}}{\centering unseen input} \\
\cline{1-4}\cdashline{5-7}
\end{tabular}
\par\vspace{12pt}
\noindent
``Current'' is Current location in the byte stream. \\
``ALA'' is the Allowed Lexeme Anchor. \\
``TB'' is the Toktrie Base. \\
\vspace{6pt}
\caption{Input structure}
\label{fig:input-1}
\end{figure}

\FloatBarrier

As a reminder, in performing the mask computation we simulate byte streams
which are divided into tokens,
and check them for acceptability against a parser which divides that same byte stream into lexemes.
From the parser
we are given a list of acceptable lexemes.
We want to use this to determine which tokens are acceptable.
Once we know which tokens are acceptable,
we can produce a token mask,
which in turn will allow us to sample a token.

To determine the acceptable tokens, we travserse a trie of the tokens,
implicitly checking the tokens against all possible byte streams.
Every node in the trie traversal implies a single byte stream,
which may be seen as structured according to the scheme in
Figure \ref{fig:input-1}
on page \pageref{fig:input-1}.

In our byte stream diagrams, we have identified
\begin{itemize}
	\item the most recently accepted lexeme,
	\item the most recently committed token,
	\item a lexeme in progress (aka the ''working lexeme''), and
	\item a token in progress (aka the ''working token'').
\end{itemize}
In the byte stream, we note three locations of special significance:
\begin{itemize}
	\item The current location, which is the byte location location
		implied by our token trie traversal.  We abbreviate this
		as \V{Current}.
	\item The ``toktrie base'', that is, the start location for our toktrie
		traversal, which we abbreviate \V{TB}.
		\V{TB} is at the end of the last committed token,
		if it exists, and at the start of parsing otherwise.
	\item The location for which we calculated the allowed lexemes,
		that is, the ``allowed lexeme location''.
		We abbreviate this as \V{ALA}.
		\V{ALA} is at the end of the last accepted lexeme,
		if it exists, and at the start of parsing otherwise.
\end{itemize}

It is always the case the current location is at or after TB:
\begin{equation}
\label{eq:invariant-1}
\V{TB} \le \V{Current}.
\end{equation}
It is also always the case the current location
is at or after the ALA:
\begin{equation}
\label{eq:invariant-2}
\V{ALA} \le \V{Current}.
\end{equation}

Our byte stream diagrams will make some assumptions
without loss of generality.
We will assume
there is a last committed token and
a last accepted lexeme.
And, while the invariants of
equation \ref{eq:invariant-1} on page \pageref{eq:invariant-1} and
equation \ref{eq:invariant-2} on page \pageref{eq:invariant-2}
will always be obeyed,
otherwise our byte stream diagrams may make arbitrary
assumptions about the relative length of the tokens and lexemes.
For example, Figure \ref{fig:input-2}
on page \pageref{fig:input-2}
is similar to Figure \ref{fig:input-1}
on page \pageref{fig:input-1},
except that, while Figure \ref{fig:input-1} shows a working
lexeme that ends {\bf before} the working token,
Figure \ref{fig:input-1} shows a working
token that ends {\bf after} the working token.

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
\multicolumn{2}{c}{TB} &
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\rule{30pt}{0pt} &
\multicolumn{2}{c|}{ALA} &
\multicolumn{2}{c}{Current} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\multicolumn{2}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\cline{1-4}\cdashline{5-7}
	\multicolumn{2}{|p{72pt}|}{\centering \hyphenpenalty=10000 accepted lexeme} &
	% 30pt * 5 + 12pt * 4 = 198pt
	\multicolumn{5}{|p{198pt};{2pt/2pt}}{\centering\vspace{-1pt}working lexeme} \\
\cline{1-4}\cdashline{5-7}
\rule{30pt}{0pt} &
	\multicolumn{2}{|p{72pt}|}{\centering committed token} &
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working token} &
        \rule{30pt}{0pt} \\
\cline{1-4}\cdashline{5-7}
	\multicolumn{4}{|p{156pt}|}{\centering seen input} &
	\multicolumn{3}{|p{114pt}}{\centering unseen input} \\
\cline{1-4}\cdashline{5-7}
\end{tabular}
\vspace{6pt}
\caption{Input structure with long working lexeme}
\label{fig:input-2}
\end{figure}

The \V{ALA} may occur before or after the \V{TB}.
Where
\begin{equation}
\label{eq:lexeme-offset}
	\V{off} = \V{TB} \subtract \V{ALA},
\end{equation}
we call \V{off} the
{\bf lexeme offset}\label{loc:lexeme-offset}.

\section{Lexeme slices}

Lexeme slices are like string slices, and are represented similarly,
so that $\Velement{lex}{0,2}$ is a lexeme which matches the first two
characters of a string which matches \V{lex} and $\Velement{lex}{2,3}$
is a lexeme which matches the third character of a string which matches
\V{lex}.

Not all lexemes can be sliced conveniently.
When a lexeme is hard to slice,
we say that it is {\bf tough}.
We consider cases.

\subsection{Fixed strings}

Where a lexeme is a fixed string,
its slices {\bf are} string slices,
and can be easily computed.
In practice, most
lexemes are fixed strings.

\subsection{Regular expressions without Kleene stars}

When a lexeme has no Kleene star,
it is also easy to compute its slices.

\subsection{Regular expressions with Kleene stars}

Lexemes with Kleene stars can be tough,\footnote{%
	Note that we cannot use derivatives where
	the lexeme offset \eqref{eq:lexeme-offset}
	is positive, as in Figure \ref{fig:input-1}
	on page \pageref{fig:input-1}.
	Computing a derivative require knowing the exact byte
	string that precedes the derivative.
	We need to compute slices which are invariant
	for every occurrence of a Ruby triple \eqref{eq:ruby-triple}
	throughout the parse, and therefore
	for which the byte strings which preceed
	the Ruby triple may vary.
	However, where the lexeme offset is negative,
	as in Figure \ref{fig:input-3}
	on page \pageref{fig:input-3},
	the preceeding byte string is entirely in the token trie,
	and so is invariant for every Ruby triple.
	We could therefore make use of derivatives in
	the case where lexeme offsets are negative.}
but many are easily sliceable.
For example, if $\V{lex} = \texttt{/a*/}$, then
$\Velement{lex}{0,2} = \texttt{/aa/}$
and
$\Velement{lex}{2,3} = \texttt{/a/}$.

\subsection{Other lexemes}

Lexemes can be many other things besides regular expressions,
including subgrammars.
Some of these problematic lexemes may be sliceable nonetheless.
For example, they could explicitly specify,
how they can be sliced.
But many of these will be tough lexemes.

\section{Lexeme slices and token trie nodes}

In the context of token trie nodes,
lexeme slices anchored around the current token
trie node are of special interest.
We call these ``anchored lexeme slices''.
Anchored lexeme slices can be conveniently represented as a duple.

Recall (equation \ref{eq:lexeme-offset} on page \pageref{loc:lexeme-offset})
that we defined the lexeme offset as \V{off}
where $\V{off} = \V{TB} \subtract \V{ALA}.$
An anchored lexeme slice can be represented as
as $(\V{lex}, \V{off})$.

For convenience in defining, anchored lexeme slices
we set out some definitions.
Let \V{node} be a token trie node,
let \VFA{path}{node} be the path from the trie root
to \V{node}, treated as a vector of nodes.
We represent the length of \VFA{path}{node}
as \size{\VFA{path}{node}}.
Abusing notation a bit, we also write
\size{\VFA{path}{node}} as \Vsize{node}.
\Vsize{node} is also called the ``depth''
of \V{node}.
The node vector \VFA{path}{node}, mapped to
a string of bytes, is \FA{toString}{\VFA{path}{node}}.
We usually overload the notation
and write \FA{toString}{\VFA{path}{node}}
as \VFA{toString}{node}.

If \V{off} is positive, so that $\V{TB} > \V{ALA}$,
then
\[
	\V{lex}, \V{off}) = \Velement{lex}{\V{off}, \Vsize{node}}.
\]
If \V{off} is negative, so that $\V{ALA} > \V{TB}$,
then
\[
	(\V{lex}, \V{off}) = \Velement{lex}{0, \Vsize{node}}.
\]

\section{Token-consistent lexemes}

At the start of parse, and after every lexeme,
there is a set of ``allowed lexemes''.
The allowed lexemes are those which, if accepted,
can result in a valid parse,
according to the parser.
{\tt llguidance} already calculates the allowed lexemes for its \url{allowed\_lexemes} vector.

Recall the location for which the allowed lexemes
are calculated is called the ALA.
The location where the last committed token ends
is the TB.
If $\V{TB} > \V{ALA}$, it becomes possible
that an allowed lexeme might be inconsistent
with the committed tokens.

For example, if the lexemes {\tt foreach} and {\tt while} are
allowed at location $\ell$,
and the token {\tt "wh"} is committed, starting at
$\ell$,
then only the lexeme {\tt while} will be consistent with
the committed tokens.
While according to the parser, the lexeme {\tt foreach}, if accepted,
could result in a valid parse,
the lexeme {\tt foreach} would be inconsistent with the
committed tokens
and would cause the parse to become inconsistent,
and/or fail.

We therefore, whenever we commit a token,
calculate the subset of the allowed lexemes
which can still produce a parse both in terms
of the parser and the tokenization.
We call these the ``token-consistent lexemes''.

The token-consistent lexemes might be equivalent
to the allowed lexemes.
When $\V{TB} \le \V{ALA}$, this will always be
the case.

\section{Stage 1: Calculating the allowed bytes}

We set out our algorithm in two stages.
While each stage does result in an optimization,
our separation of the algorithm into stages
is to simplify the presentation.
It is not (at last not necessarily)
a suggestion for gradual implementation.

In Stage 1, we go from the allowed lexemes
to the allowed bytes of tokens,
and use this to eliminate calls to
\url{try\_push\_byte}.
We refer to the code snippet\footnote{%
    This code snippet is similar to, and taken from,
    \url{https://github.com/microsoft/llguidance/blob/0ca091a701a50134e0503fa03c5c12b206e182a3/toktrie/src/toktree.rs\#L945}.
}
in Figure \ref{fig:try-push-byte} on page \pageref{fig:try-push-byte}.
\begin{figure}
\begin{lstlisting}
    while p < endp {
        r.pop_bytes(next_pop);
        let n = &self.nodes[p];
        let b = n.byte();
        // Click ruby slipper heels here
        if r.try_push_byte(b) {
            toks.allow_token(n.token_id().
                unwrap_or(defl_tok)
            );
            // ... stuff here ...
        } else {
            // ... other stuff here ...
        }
    }
\end{lstlisting}
\caption{Ruby Slippers Click Point%
    \label{fig:try-push-byte}%
}
\end{figure}

Let \V{lexes} be the set of allowed lexemes at
the ``click point'' in
Figure \ref{fig:try-push-byte} on page \pageref{fig:try-push-byte}.
The test \url{try\_push\_byte} will be true only if,
\begin{equation}
	\label{eq:click-point-test}
	\exists \; \V{lex} \in \V{lexes} \; \mid \; \VFA{toString}{node} \, \sim \, (\V{lex}, \V{off}),
\end{equation}
where $\V{s} \, \sim \, \V{recce}$ is true iff the string \V{s} matches the recognizer
\V{recce}.
When \eqref{eq:click-point-test} is false,
the call to \url{try\_push\_byte} can be skipped.
For a ``tough'' lexeme \V{tuff}, the recognizer $(\V{tuff}, \V{off})$
always matches, so that the call
to \url{try\_push\_byte} is forced.

In practice,
most lexemes are sliceable,
and allowed and possible lexemes are sparse.
So even at Stage 1,
this optimization should be significant.
But a more substantial payoff should be realized in Stage 2.

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
        \multicolumn{2}{c}{ALA} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} \\
	% ===
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
        \multicolumn{2}{c|}{TB} &
        \multicolumn{2}{c}{Current} \\
	% ===
	\rule{30pt}{0pt} &
        \multicolumn{2}{c|}{} &
	\multicolumn{1}{c|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{c|}{\rule{30pt}{0pt}} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} \\
	% ===
\cline{1-5}\cdashline{6-7}
\multicolumn{4}{|c|}{accepted lexeme} &
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working lexeme} \\
\cline{1-5}\cdashline{6-7}
        \rule{30pt}{0pt} &
        \multicolumn{2}{|p{6em}|}{\centering committed token} &
        \multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working token}
        \\
	% ===
\cline{1-5}\cdashline{6-7}
\multicolumn{5}{|c|}{seen input} &
        \multicolumn{2}{|c}{unseen input} \\
\cline{1-5}\cdashline{6-7}
\end{tabular}
\vspace{6pt}
\caption{Input structure with token split between lexemes}
\label{fig:input-3}
\end{figure}

\section{Caching}
\label{sec:caching}

Stage 1 relies on, for every node \V{node},
begin knowing the value of
\[
	\VFA{toString}{node} \, \sim \, (\V{lex}, \V{off}).
\]
To really achieve optimization,
these results should be cached,
so that
\begin{equation}
\label{eq:ruby-triple}
	\FA{Ruby}{\V{node}, \V{lex}, \V{off}}
\end{equation}
is a boolean partial function.
We will call
\[
	(\V{node}, \V{lex}, \V{off})
\]
a ``Ruby triple'',
so that \Fname{Ruby} in \eqref{eq:ruby-triple}
is a partial function from Ruby triples to booleans.

It may be best to vary the caching strategy.
For small absolute values of \V{off}, the cache
will be dense,
and the likelihood of hits in a fully pre-populated
cache will be high.
In this case,
pre-computation at grammar compile time makes sense.
Computing many of these values in a single token trie
traversal would allow for optimizations.

Lexemes may be of arbitrary length,
so pre-computation of all Ruby triple values
is not just costly, but impossible.
Further,
in practical applications,
the users of
very large absolute values of \V{off}
will be very long strings or here-documents.
These are unlikely to be repeated,
so that the likelihood of cache hits would
be very low.
Therefore, for absolute values of \V{off}
above a certain threshold,
it is likely to make sense to not cache the
value of Ruby triples.

We have spoken of varying the caching strategy according
to the value of \V{off}.
It may also make sense to vary the caching strategy by
lexeme.
For example, we may want a different caching strategy
depending on whether a lexeme is sliceable or tough.

\section{Stage 2: Track ends of lexeme}
\label{sec:stage2}

We call a location where a lexeme can end,
an ``end of lexeme'', or EOL.
In Stage 2, we build on Stage 1 by tracking EOLs.
In cases where \url{try\_push\_byte(b)}
is called when we are not at an EOL,
\url{try\_push\_byte(b)} simply stores byte \V{b}.
If we skip the call to \url{try\_push\_byte(b)},
but turn out to need the value of byte \V{b}
later, we can find it from context.

Note that,
while Stage 1 required no changes in the logic of
\url{try\_push\_byte},
Stage 2 may require some modifications.

\section{Forcing}

In this document we say we are ``lexeme forcing'' when there is only one
relevant lexeme at a location.
The optimization described here detects situations where lexemes can be
forced and handles their tokenization in the same way as it handles
the tokenization of other lexemes.

\FloatBarrier

{
\clearpage

% Ragged right, do not hyphenate.
\RaggedRight
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Silence current hbox warnings, but allow them if
% badness increases further
\hbadness=2000

\begin{thebibliography}{10}

\bibitem{Kegler2011a}
\newblock{Jeffrey Kegler.}
\newblock{``What is the Marpa algorithm?''.}
\newblock{\url{https://blogs.perl.org/users/jeffrey_kegler/2011/11/what-is-the-marpa-algorithm.html}.
   Retrieved 2 Dec, 2024.}

\bibitem{Kegler2011b}
\newblock{Jeffrey Kegler.}
\newblock{``Marpa and the Ruby Slippers''.}
\newblock{\url{http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2011/11/marpa-and-the-ruby-slippers.html}.
   Retrieved 2 Dec, 2024.}

\bibitem{Kegler2023}
\newblock{Jeffrey Kegler.}
\newblock{``Marpa, A practical general parser: the recognizer''.}
\newblock{\url{https://arxiv.org/abs/1910.08129}.
   Retrieved 2 Dec, 2024.}

\bibitem{llguidance}
\newblock{llguidance.}
\newblock{"Low-level guidance parser."}
\newblock{
  \url{https://github.com/microsoft/llguidance}.
  Retrieved 2 Dec, 2024.}

\end{thebibliography}

} % RaggedRight

% \clearpage
% \phantomsection
% \listofalgorithms

\ifdraft{
    \clearpage
    \phantomsection
    \listoftodos
}{}

\end{document}
