\documentclass[draft,12pt]{amsart}

\usepackage[final]{listings}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{needspace}

\usepackage{amssymb}
\usepackage{wasysym} % Used for \Circle
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{url}
\usepackage[final]{hyperref}
\usepackage{placeins}
\usepackage{mfirstuc}
\usepackage{ifdraft}
\usepackage[obeyDraft]{todonotes}
\usepackage{datetime2}
\ifdraft{
    \usepackage{draftwatermark}
}{}

% Fix conflict between todonotes and amsart packages
% See http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/todonotes/todonotes.pdf,
% p. 10.
\makeatletter
\providecommand\@dotsep{5}
\def\listtodoname{List of TODOs}
\def\listoftodos{\@starttoc{tdo}\listtodoname}
\makeatother

\makeatletter
\renewcommand{\listofalgorithms}{\@starttoc{loa}{List of algorithms}}
\let\l@algorithm=\l@figure
\makeatother

% Re hbox's, see
% https://tex.stackexchange.com/questions/241343/what-is-the-meaning-of-fussy-sloppy-emergencystretch-tolerance-hbadness
\raggedbottom

% Replaced by todonotes package
% \newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}

\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}
\newcommand{\mymathbin}[1]{\mathbin{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\mult}{\mathbin{\ast}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}\xspace}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

\newcommand{\lastix}[1]{\ensuremath{\##1}}
\newcommand{\Vlastix}[1]{\ensuremath{\lastix{\V{#1}}}}
\newcommand{\VlastElement}[1]{\Velement{#1}{\Vlastix{#1}}}
\newcommand{\element}[2]{\ensuremath{\mathop{#1}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\Velement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\VVelement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[ \V{#2}
    \right]\mathclose{} }}

\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\mcolon}{\mathrel:}
\newcommand{\mcoloncolon}{\mathrel{\vcenter{\hbox{$::$}}}}

\newcommand{\suchthat}{\mcoloncolon}
\newcommand{\quantify}[3]{% quantifier, binding, predicate
    \ensuremath{\mathrel{#1}#2 \; \suchthat \; #3}%
}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\V}[1]{\ensuremath{\texttt{\mbox{#1}}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\Rightarrow}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

% For the type subscript for powersets
\newcommand{\setOf}[1]{{{#1}^{\mbox{\normalsize $\ast$}}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }

% use internal counter of algorithmicx
\newcommand{\algnested}{ %
  \makeatletter\ALG@nested\makeatother %
}

\newcommand{\algstrut}{\rule{0pt}{.85\baselineskip}}

\newcommand{\myparbox}[2][\algparwidth]{%
  \parbox[t]{#1}{%
  % \linespread{1.15}\selectfont
  \ignorespaces#2\par%
  }%
}

\newcommand{\parcomment}[2]{\hspace{\dimexpr \algorithmicindent * #1}$\triangleright$ #2}

% \newcommand{\rawparcomment}[2][\algparwidth]{%
  % \myparbox[#1]{#2}%
% }
% \newcommand{\parcomment}[2][\algparwidth]{%
  % \myparbox[#1]{$\triangleright$ #2}%
% }
% \newcommand{\parcomment}[1]{\hspace{\dimexpr \algorithmicindent - \labelwidth }$\triangleright$
  % \protect\parbox[t]{\dimexpr \textwidth - (\algorithmicindent - \labelwidth)}{#1}%
% }

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newcommand{\mname}[1]{\mbox{\sf #1}}
% [O]perator [A]pplication
\newcommand{\Oname}[1]{\mname{#1}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\OA}[2]{\ensuremath{%
    \mathop{\Oname{#1}}\mathopen{}\left(#2\right)\mathclose{}%
}}
\newcommand{\VOA}[2]{\OA{#1}{\V{#2}}}
% [F]unction [A]pplication, replaces \myfn
\newcommand{\smallrawfn}[2]{\ensuremath{\mathop{#1}(#2)}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\rawfn}[2]{\ensuremath{\mathop{#1}\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\Fname}[1]{\V{#1}}
\newcommand{\FA}[2]{\ensuremath{\rawfn{\mathop{\Fname{#1}}}{#2}}}
\newcommand{\VFA}[2]{\FA{#1}{\V{#2}}}

% * Reference meta tags ("classifying prefixes") use in this document:
% * From fancyref:
% Chapter chap:
% Section sec:
% Equation eq:
% Figure fig:
% Table tab:
% Enumeration enum:
% Footnote fn:
%
% * Not in fancyref
% Definition df:
% Epigraph epi:
% Theorem th:
% Lemma lm:
% Remark rm:
% Observation obs:
% Example ex:
% Page page:

% The "page:" metatag is for labels intended as the target of
% page references.

% The auto-placement of qed's fails a lot.
% Semi-manual replacement using \qedhere often fails as well.
% I went with % 100% manual placement,
% which in fact is not any more trouble,
% and which is the only way to guarantee the QED's wind up where I
% want them.

% Try to gather all the stuff to do with theoremoids here
% Start off with theorems, then the other theoremoids,
% mostly alphabetically.

\renewcommand{\qedsymbol}{}
\newcommand{\myqed}{%
  \ifmmode\square
  \else$\square$
  \fi%
}

\newcommand{\colonpageref}[1]{%
 \ifthenelse{\equal{\pageref{#1}}{\thepage}}%
  {}{{:}\pageref*{#1}}%
}

% genref -- generic reference
\newcommand{\genref}[3][]{%
    \ifthenelse{\equal{#1}{}}{}{%
      \ifmmode \text{``#1''}%
      \else``#1'' %
      \fi%
    }%
    (\ensuremath{% To cover both cases, force math mode
        \hyperref[#2]{%
            \textrm{#3}\ref*{#2}\colonpageref{#2}%
        }%
    })%
}

\theoremstyle{definition}

% theoremoids: Th

\newtheorem{theorem}{Theorem}
\newcommand{\Thref}[2][]{%
    \genref[#1]{th:#2}{Th}%
}
% some day delete \longThref
\newcommand{\longThref}[2]{%
  \ifmmode \text{``#1''\Thref{#2}}%
  \else``#1'' \Thref{#2}%
  \fi%
}
% "full" ref -- always includes page
\newcommand{\ThFref}[1]{(\textrm{Th}\ref{#1}:\pageref{#1})}
\newcommand{\thEnd}{\Circle}

% theoremoids: Df

\newtheorem{definition}[theorem]{Definition}
\newcommand{\Dfref}[2][]{%
    \genref[#1]{df:#2}{Df}%
}
\newcommand{\dfEnd}{\Circle}

% theoremoids: Ex

\newtheorem{example}[theorem]{Example}
\newcommand{\Exref}[1]{\genref{ex:#1}{Ex}}
\newcommand{\exEnd}{\Circle}

% theoremoids: Ob

\newtheorem{observation}[theorem]{Observation}
\newcommand{\Obref}[1]{\genref{obs:#1}{Ob}}
\newcommand{\obEnd}{\Circle}

% theoremoids: Rm

\newtheorem{remark}[theorem]{Remark}
\newcommand{\Rmref}[2][]{\genref[#1]{rm:#2}{Rm}}
\newcommand{\rmEnd}{\Circle}

% local theoremoids: Lm

\newtheorem{lemma}[theorem]{Lemma}
\newcommand{\Lmref}[2][]{\genref[#1]{lm:#2}{Lm}}
% some day delete \longLmref
\newcommand{\longLmref}[2]{%
  \ifmmode \text{``#1''\Lmref{#2}}%
  \else``#1'' \Lmref{#2}%
  \fi%
}
\newcommand{\lmEnd}{\thEnd}
\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

% I like to silence underfull hbox messages.
% This is syntactic sugar for doing that.
\newenvironment{MYsloppy}[1][3em]{\par\tolerance9999 \emergencystretch#1\relax}{\par}
% This form allows the resetting of hbadness, which may be necessary to
% shut up an "underfull" warning.
\newenvironment{MYsloppyB}[2]{\par\tolerance9999 \emergencystretch#1 \hbadness#2\relax}{\par}
% This form is for a trick:  Tolerances are evaluated line by line, so a tolerance
% less than 9999 may produce a better looking paragraph and reduce the "badness".
\newenvironment{MYsloppyC}[3]{\par\tolerance#3 \emergencystretch#1 \hbadness#2\relax}{\par}

\title
  [Masking Scheme]
  {A scheme for optimizing mask computation}

\author{Jeffrey Kegler}
\ifdraft{
    % legacy draftwatermark options -- update?
    \SetWatermarkLightness{0}
    \SetWatermarkText{DRAFT \DTMnow{} DRAFT}
    \SetWatermarkAngle{0}
    % \SetWatermarkScale{1}
    % \SetWatermarkHorCenter{1.5in}
    \SetWatermarkVerCenter{.5in}
}{}
\ifdraft{ \thanks{DRAFT as of \DTMnow} }{}
\date{\DTMnow.}

\newcommand{\tpb}{\url{try\_push\_byte}\xspace}
\newcommand{\tpbb}{\url{try\_push\_byte(b)}\xspace}
\newcommand{\al}{\url{allowed\_lexemes}\xspace}
\newcommand{\llguidance}{{\tt llguidance}\xspace}

\begin{document}

\maketitle

\section{About this document}
\label{sec:about-me}

This document outlines a method for optimizing mask computation in
Guidance.
It assumes some familiarity with the code in
\llguidance\cite{llguidance}.

\section{Conventions}
\begin{itemize}
\item ``Iff'' (or ``iff'') abbreviates
	``if and only if''.
\item Ranges are inclusive start,
	exclusive end, so that, for example,
		\begin{gather*}
			\element{(\texttt{"abcd"})}{0,2} = \texttt{"ab"} \text{, and} \\
			\element{(\texttt{"abcd"})}{2,3} = \texttt{"c"}.
		\end{gather*}
\end{itemize}

\section{Summary}

\FloatBarrier

The optimization suggested here works by reducing the number of calls
of \tpb
and use this to eliminate calls to
\tpb.\footnote{%
See Figure \ref{fig:try-push-byte} on page~\pageref{fig:try-push-byte},
    which contains a code snippet that is similar to, and taken from,
    \url{https://github.com/microsoft/llguidance/blob/0ca091a701a50134e0503fa03c5c12b206e182a3/toktrie/src/toktree.rs\#L945}.
}
\begin{figure}
\begin{lstlisting}
    while p < endp {
        r.pop_bytes(next_pop);
        let n = &self.nodes[p];
        let b = n.byte();
        // Click ruby slipper heels here
        if r.try_push_byte(b) {
            toks.allow_token(n.token_id().
                unwrap_or(defl_tok)
            );
            // ... stuff here ...
        } else {
            // ... other stuff here ...
        }
    }
\end{lstlisting}
\caption{Ruby Slippers Click Point%
    \label{fig:try-push-byte}%
}
\end{figure}
The optimization prefaces
(and hopefully usually replaces)
the call to \tpb with a small
set of calls (one for each of a subset of the allowed
lexemes) to a cache containing
byte pairs.
The set, and the byte pairs,
are free of per-location charge (FOPC).

The byte pairs are FOPC
because they do not vary by location,
and therefore may be calculated once
and cached.
The allowed lexemes set is FOPC
because it will have already been calculated for each location
for other purposes, and therefore impose
no new costs.\footnote{%
        We ignore for the moment a slight complication
        of little cost,
        which requires us sometimes to use the token-consistent lexemes,
        a subset of the allowed lexemes.
        See Section \ref{sec:-token-consistent lexemes}
        on page \pageref{sec:-token-consistent lexemes}.
}

The byte pairs guide traversal of the token trie.
There is a GO-bit which, if true tells us to continue
trie traversal below the current node and,
if false, tells us to not traverse more deeply into
the token trie.
In addition, there is an EOL-bit, which tells us if
we are at the end of a lexeme (aka EOL).
We use the GO-bit to avoid most trie traversal,
without incurring the cost of a \tpb call.
We use the EOL-bit to limit calls to \tpb to
situations where we are at the end of a lexeme.

Calls to \tpb
need only be made at end of lexeme,
and for tough lexemes.
\todo{Where will tough lexemes be discussed?}
Tough lexemes should be sparse and will be defined
and discussed later\footnote{Where?}.

\section{The basic ideas}
\subsection{Location independence}

The first basic idea behind the suggestions in this document is
the well-known one of taking invariant code called repeatedly,
and arranging for it to be called only once.\cite{wiki-loop-invariant}
In our case, instead of moving the invariant code before the repeating
logic,
we may compute its value once and cache it.\footnote{%
	See Section \ref{sec:caching}
	on page~\pageref{sec:caching}.}

To be ``invariant'' in a sense useful for us,
the result logic must be ``location-independent'' (LI)
in the sense that it produces the same result
regardless of the parse location at which it is called.
Logic which is not location-independent is
called ``location-dependent''.

\subsection{The Ruby Slippers}

The second basic idea behind this document is more
aggressive use of the Ruby Slippers.
The Ruby Slippers is
a parsing technique discovered independently by Jeffrey Kegler%
\cite{Kegler2011a}\cite{Kegler2011b}\cite{Kegler2023}
and
Michal Moskal\cite{llguidance}.
It takes advantage of the ability of certain variants of the Earley
parser to tell the application
exactly which terminals will be acceptable at
any point in the parse.
This allows the application to change the lexeme stream to match
the parser's expectations.
The application could, for example, invent a lexeme on the fly
to accommodate the parser.\footnote{%
    The name ``Ruby Slippers'' comes from a incident in the movie
    {\it Wizard of Oz}.  In it Dorothy, the protagonist, is trying
    to return to her home in Kansas.  After many adventures she
    discovers that a pair of ruby slippers, which she has been
    wearing throughout the movie, grant her this ability.
    She has merely to click her heels and wish.}

The Ruby Slippers are already in use
in \llguidance.
An \al vector is computed,
and applied at certain points.

Looking ahead
in this document, we are going to claim
that the Ruby Slippers can be used earlier and more extensively,
and that most of the work of applying the
Ruby Slippers is location-independent.
It is hoped that this will allow considerable savings in time.

The need to access information about the stack is often
seen as an obstacle to location-independence:
Any computation which refers to the stack, it is thought,
must be location-dependent.\footnote{%
	For example,
	from Section 1, ``Introduction'' in \cite{XGrammar2024}:
	``CFG interpretation
	requires a stack state that tracks the recursive rules matched
	so far, making it impossible to precompute and cache all
	combinatorial combinations of stack patterns ahead of time.''}
We can use the Ruby Slippers, however, to make
most computation involving the stack location-independent.

To do this, we note that the only stack information we care
about is the list of ``allowed lexemes'', and information
about them.
The information about the allowed lexemes is location-independent,
and, in a Ruby-Slippers-ready implementation of the Earley algorithm,
like \llguidance,
the list of allowed lexemes is quickly found.
In fact, in \llguidance, the list of allowed lexemes
is already computed as \al,
so that it comes at literally zero cost.
For our purposes, therefore, the only cause of location-dependence will
be reference to portions of the input too early
to be found in the token trie.

The results of the LI logic are booleans,
which are cached and used to avoid
calls to \tpbb.
Using this optimization,
\tpbb typically will only be called
when a new lexeme is ready to be accepted.
The exception is a minority of instances,
where the logic necessary to determine when
a new lexeme is ready to be accepted
cannot be made LI.
In those cases,
the cached booleans will be overapproximations,
and \tpbb will be used as a fallback.

\FloatBarrier

\needspace{3in}

\section{The structure of the byte stream}
\label{sec:byte-stream}

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
	\multicolumn{2}{c}{TB} &
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\rule{30pt}{0pt} &
	\multicolumn{2}{c|}{ALA} &
\multicolumn{2}{p{72pt}}{\centering Current} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
	\multicolumn{2}{p{72pt}|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{p{30pt}|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{p{30pt}|}{\rule{30pt}{0pt}} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-5}
	\multicolumn{2}{|p{72pt}|}{\centering \hyphenpenalty=10000 accepted lexeme} &
	% 30pt * 5 + 12pt * 4 = 198pt
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt} working lexeme}
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-6}
\rule{30pt}{0pt} &
	\multicolumn{2}{|p{72pt}|}{\centering committed token} &
	% 30pt * 3 + 12pt * 2 = 114pt
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt} working token} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-7}
	\multicolumn{4}{|p{156pt}|}{\centering seen input} &
	\multicolumn{3}{|p{114pt}}{\centering unseen input} \\
\cline{1-4}\cdashline{5-7}
\end{tabular}
\par\vspace{12pt}
\noindent
``Current'' is Current location in the byte stream. \\
``ALA'' is the Allowed Lexeme Anchor. \\
``TB'' is the Toktrie Base. \\
\vspace{6pt}
\caption{Input structure}
\label{fig:input-1}
\end{figure}

\FloatBarrier

We assume some knowledge of the codebase of \llguidance.
As a reminder, in performing the mask computation we simulate byte streams
which are divided into tokens,
and check them for acceptability against a parser which divides that same byte stream into lexemes.
From the parser
we are given a list of acceptable lexemes.
We want to use this to determine which tokens are consistent with the parse so far.
Once we know which tokens are parse-consistent,
we can produce a token mask,
which in turn will allow us to sample a token.

To determine the acceptable tokens, we traverse a trie of the tokens,
implicitly checking the tokens against all possible byte streams.
Every node in the trie traversal implies a single byte stream,
which may be seen as structured according to the scheme in
Figure \ref{fig:input-1}
on page~\pageref{fig:input-1}.

In Figure \ref{fig:input-1}
and our other
byte stream diagrams, we have identified
\begin{itemize}
	\item the most recently accepted lexeme,
	\item the most recently committed token,
	\item a lexeme in progress (aka the ''working lexeme''), and
	\item a token in progress (aka the ''working token'').
\end{itemize}
In the byte stream, we note three locations of special significance:
\begin{itemize}
	\item The current location, which is the location in the input stream
		corresponding to our current position in the our token trie traversal.
		We abbreviate this as \V{Current}.
	\item The ``toktrie base'', that is, the start location for our toktrie
		traversal, which we abbreviate \V{TB}.
		\V{TB} is at the end of the last committed token,
		if it exists, and at the start of parsing otherwise.
	\item The location for which we calculated the allowed lexemes,
		that is, the ``allowed lexeme location''.
		We abbreviate this as \V{ALA}.
		\V{ALA} is at the end of the last accepted lexeme,
		if it exists, and at the start of parsing otherwise.
\end{itemize}

It is always the case the current location is at or after TB:
\begin{equation}
\label{eq:invariant-1}
\V{TB} \le \V{Current}.
\end{equation}
It is also always the case the current location
is at or after the ALA:
\begin{equation}
\label{eq:invariant-2}
\V{ALA} \le \V{Current}.
\end{equation}

For ease of drawing,
and without loss of generality,
our byte stream diagrams
will make several assumptions.
We will assume
there is a last committed token and
a last accepted lexeme.
And, while the invariants of
\eqref{eq:invariant-1} and \eqref{eq:invariant-2}
will always be obeyed,
otherwise our byte stream diagrams may make arbitrary
assumptions about the relative length of the tokens and lexemes.
For example, Figure \ref{fig:input-2}
on page~\pageref{fig:input-2}
is similar to Figure \ref{fig:input-1}
on page~\pageref{fig:input-1},
except that, while Figure \ref{fig:input-1} shows a working
lexeme that ends {\bf before} the working token,
Figure \ref{fig:input-2} shows a working
lexeme that ends {\bf after} the working token.

The \V{ALA} may occur before or after the \V{TB}.
Where
\begin{equation}
\label{eq:lexeme-offset}
	\V{off} = \V{TB} \subtract \V{ALA},
\end{equation}
we call \V{off} the
{\bf lexeme offset}\label{loc:lexeme-offset}.

\begin{definition}[Token trie variables]
        \label{df:token-trie}
\noindent
Let \V{node} be a token trie node.
Then the following are true:
\begin{MYsloppy}
\begin{itemize}
\item \VFA{path}{node} is the path from the trie root
to \V{node}, treated as a vector of nodes.
\item
\size{\VFA{path}{node}}
is the length of \VFA{path}{node}.
Abusing notation a bit, we also write
\size{\VFA{path}{node}} as \Vsize{node}.
\Vsize{node} is also called the ``depth''
of \V{node}.
\item
\FA{toString}{\VFA{path}{node}} is
the node vector \VFA{path}{node}, mapped to
a sequence (aka string) of bytes.
We usually overload the notation
and abbreviate \FA{toString}{\VFA{path}{node}}
as \VFA{toString}{node}.
\dfEnd
\end{itemize}
\end{MYsloppy}
\end{definition}

\section{Lexeme slices}

Lexeme slices are like string slices, and are represented similarly,
so that $\Velement{lex}{0,2}$ is a lexeme which matches the first two
characters of a string which matches \V{lex} and $\Velement{lex}{2,3}$
is a lexeme which matches the third character of a string which matches
\V{lex}.

In this document, we refer to a lexeme slice as location-independent (LI)
if the slice matches correctly regardless of any earlier bytes in the
string.
Here are some examples of lexeme slices:

\begin{equation}
	\label{eq:ex-fixed}
    \element{(\texttt{/abcd/})}{2,3} = \texttt{/c/}
\end{equation}
    is an LI slice, because the string \texttt{"c"}
    will always match the third character of any string
    which matches $\texttt{/abcd/}$.

\begin{equation}
	\label{eq:ex-no-slice}
    \element{(\texttt{/(aaaaaa|bbbbbb)/})}{2,3}
\end{equation}
\MYsloppy{%
\noindent
does not have an LI slice.
To see why, we first note that
    the third character of \eqref{eq:ex-no-slice}
    must be either
    \texttt{"a"} or \texttt{"b"}.
    If the third character is \texttt{"a"},
    \eqref{eq:ex-no-slice}
    will not match the third character of
		\texttt{"bbbbbb"} even though
		\texttt{"bbbbbb"} matches
    \texttt{/(aaaaaa|bbbbbb)/}.
    If the third character is \texttt{"b"},
    \eqref{eq:ex-no-slice} will not match the third character of
		\texttt{"aaaaaa"} even though
		\texttt{"aaaaaa"} matches
    \texttt{/(aaaaaa|bbbbbb)/}.
    The lexeme $\element{(\texttt{/(aaaaaa|bbbbbb)/})}{2,3}$
		can be dealt with by overapproximation
		(see Section \ref{sec:overapproximation}
		on page~\pageref{sec:overapproximation}).
}% MYsloppy

\begin{equation}
	\label{eq:ex-factored}
    \element{(\texttt{/(aaaaaa|bbbbbb)/})}{0,3}
		= \texttt{/(aaa|bbb)/}
\end{equation}
\noindent%
is an LI slice.
In fact,
because the set of prefixes of a regular language
is always a regular language,
every slice which begins at location
0 of the lexeme is an LI slice.

\begin{equation}
	\label{eq:ex-kleene}
    \element{(\texttt{/a*/})}{2,4}
		= \texttt{/aa/}
\end{equation}
\noindent%
is an LI slice.
Kleene star regular expressions often have LI slices.

\needspace{3in}
\begin{equation}
	\label{eq:ex-case-insensitive}
	\element{(\texttt{/[Ll][Oo][Rr][Ee][Mm]/})}{2,3} = \texttt{/[Rr]/}
\end{equation}
\noindent%
Case-insensitive lexemes like
\eqref{eq:ex-case-insensitive}
are not a problem for slicing.
Writing case-insensitive \texttt{/lorem/} as
\texttt{/[Ll][Oo][Rr][Ee][Mm]/},
we see that \eqref{eq:ex-case-insensitive}
is an LI slice.

If there is a range for which there is no LI slice
for a lexeme,
we say that it is ``unsliceable''.
When slicing a lexeme is problematic,
we say that the lexeme is ``tough''.

We consider cases:

\begin{itemize}
	\item Every fixed string is sliceable,
and none of them are tough.
In practice, most
lexemes are fixed strings.
\item
When a lexeme has no Kleene star,
it is sliceable by ``factoring'' it into fixed
strings.
Some factorings may be exponential, however,
so that regular expression lexemes,
even without Kleene stars, can be tough.
\item
	Not all Kleene star regular expressions
	are sliceable, but many are,
	as we saw in \eqref{eq:ex-kleene}
	on page~\pageref{eq:ex-kleene}.
\item Lexemes can be many other things besides regular expressions,
including subgrammars.
Some of these problematic lexemes may be sliceable nonetheless.
For example, the application could specify explicitly
how they can be sliced.
But many of these will be tough lexemes.
\end{itemize}
\todo{Discuss open-ended lexeme slices}

\section{Overapproximation}
\label{sec:overapproximation}

Above, we introduced the idea of tough and unsliceable lexemes.
These can be handled with overapproximation.
Our overapproximation must not reject any matching lexemes (``false negatives''),
but they may accept non-matching lexemes (``false positives'').

As an example of overapproximation, we revisit
\eqref{eq:ex-no-slice} on
page~\pageref{eq:ex-no-slice}:
\begin{equation}
	\label{eq:unsliceable}
    \element{(\texttt{/(aaaaaa|bbbbbb)/})}{2,3}
\end{equation}
is not sliceable.
But a good overapproximation to
\eqref{eq:unsliceable} is
\begin{equation}
	\label{eq:approx}
\texttt{/[ab]/},
\end{equation}
which matches either
\texttt{"a"} or \texttt{"b"}.
\eqref{eq:approx} avoids false negatives,
as required.
\eqref{eq:approx} does return false positives,
but only in the case of one byte out of
the 256 possibilities,
so the accuracy of
\eqref{eq:approx} is high.

The overapproximation of last resort is a ``wildcard'',
which matches everything.
Almost all lexemes will be sliceable in practice,
and an implementation is likely to find it most convenient
to treat all lexemes as if they were sliceable.
The wildcard approximation allows this.

\section{Token-consistent lexemes}
\label{sec:-token-consistent lexemes}

At the start of parse, and after every lexeme,
there is a set of ``allowed lexemes''.
The allowed lexemes are those which, if accepted,
can result in a valid parse,
according to the parser.
\llguidance already calculates the allowed lexemes,
which it tracks in its \al vector.

Recall the location for which the allowed lexemes
are calculated is called the ALA.
The location where the last committed token ends
is TB.
If $\V{TB} > \V{ALA}$, it becomes possible
that an allowed lexeme might be inconsistent
with the committed tokens.

For example, if the lexemes {\tt foreach} and {\tt while} are
allowed at location $\ell$,
and the token {\tt "wh"} is committed, starting at
$\ell$,
then only the lexeme {\tt while} will be consistent with
the committed tokens.
While according to the parser, the lexeme {\tt foreach}, if accepted,
could result in a valid parse,
the lexeme {\tt foreach} would be inconsistent with the
committed tokens
and would cause the parse to become inconsistent,
and/or fail.

We therefore, whenever we commit a token,
calculate the subset of the allowed lexemes
which can still produce a parse both in terms
of the parser and the tokenization.
We call these the ``token-consistent lexemes''.

The token-consistent lexemes might be equivalent
to the allowed lexemes.
When $\V{TB} \le \V{ALA}$, this will always be
the case.
A token-consistent lexeme is always an allowed lexeme.

Allowed lexemes should be sparse,
and token-consistent lexemes will be even more sparse.\footnote{%
	For \cite{XGrammar2024}
	the nearest approximation to ``token-consistent lexemes'' is
	their ``context-dependent tokens''.
	In one experiment, with a JSON parser,
	they put ``context-dependent tokens'' at "less than 1\%
	(1134 out of 128k)"
	(Section 3.1, ``Adaptive Token Mask Cache'').
	Our Earley engine has more information about the parse
	than the PDA of \cite{XGrammar2024}, so we would expect
	``token-consistent lexemes'' to be even sparser
	than ``context-dependent tokens''.
	Comparing statistics from the two approaches, however,
	is a risky business.
	We also should keep in mind that JSON is, by design,
	a very easy to parse language,
	so should naturally yield a low count of
	both ``token-consistent lexemes'' and
	``context-dependent tokens''.},

\section{Calculating LIVE and EOL at each byte location}

We must calculate a per-byte-location value for LIVE and
EOL.
Let only one lexeme, call it \V{lex},
be token-consistent
at $\ell$, a particular byte location.
Then the following are true:
\begin{itemize}
        \item The value of LIVE for $\ell$ is the value
of the LIVE-bit for \V{lex}.
        \item If the LIVE-bit for \V{lex} is true,
the value of EOL for $\ell$ is the value
of the EOL-bit for \V{lex}.
If the LIVE-bit for \V{lex} is false,
the value of EOL for $\ell$ is undefined.
\end{itemize}

The most common count of token-consistent lexemes
per byte location will be one.
But it will by no means be rare for
mulitple lexemes to be token-consistent
at a single byte location,
as we must know how to deal with that.

\section{LIVE- and EOL-bit for positive offsets}

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
\multicolumn{2}{c}{TB} &
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\rule{30pt}{0pt} &
\multicolumn{2}{c|}{ALA} &
\multicolumn{2}{c}{Current} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\multicolumn{2}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\cline{1-4}\cdashline{5-7}
	\multicolumn{2}{|p{72pt}|}{\centering \hyphenpenalty=10000 accepted lexeme} &
	% 30pt * 5 + 12pt * 4 = 198pt
	\multicolumn{5}{|p{198pt};{2pt/2pt}}{\centering\vspace{-1pt}working lexeme} \\
\cline{1-4}\cdashline{5-7}
\rule{30pt}{0pt} &
	\multicolumn{2}{|p{72pt}|}{\centering committed token} &
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working token} &
        \rule{30pt}{0pt} \\
\cline{1-4}\cdashline{5-7}
	\multicolumn{4}{|p{156pt}|}{\centering seen input} &
	\multicolumn{3}{|p{114pt}}{\centering unseen input} \\
\cline{1-4}\cdashline{5-7}
\end{tabular}
\vspace{6pt}
\caption{Input structure with long working lexeme}
\label{fig:input-2}
\end{figure}

A lexeme slice can be represented
as \FA{Slice}{\V{lex}, \V{off}}.
In this section we considered the case
where the start of the token is after the
start of the lexeme, so that the TB
is properly inside the lexeme.

Recall (equation \ref{eq:lexeme-offset} on page~\pageref{loc:lexeme-offset})
that we defined the lexeme offset as \V{off}
where
\[
	\V{off} = \V{TB} \subtract \V{ALA}.
\]
From \eqref{eq:lexeme-offset}
and Figure~\ref{fig:input-2},
it can be seen that a slice with TB at
its left has a positive offset.

Recall the definitions of \V{node}, etc.,
in Definition \Dfref{token-trie}.
We use these definitions to state the value
of the anchored lexeme slices.
If \V{off} is positive,
as in Figure~\ref{fig:input-1} on page~\pageref{fig:input-1},
so that $\V{TB} > \V{ALA}$,
\begin{equation}
\label{eq:positive-offset}
        \FA{Slice}{\V{lex}, \V{off}} = \Velement{lex}{\V{off}, \Vsize{node}+\V{off}}.
\end{equation}

In what follows we will be interested in the overlap of the lexeme
and \VFA{toString}{node}, the string implied by \V{node} in the
token trie.
We write this overlap as
\[
	\FA{overlap}{\V{node},\V{off}}
\]
If \V{off} is positive, so that $\V{TB} > \V{ALA}$,
the entire token trie overlaps
a segment of the lexeme,
so that \VFA{toString}{node} is identical to the overlap:
\begin{equation}
\label{eq:positive-overlap}
	\FA{overlap}{\V{node},\V{off}} = \VFA{toString}{node}
\end{equation}

\FloatBarrier
\section{LIVE- and EOL-bit for negative offsets}

\todo{This section is now scrambled.  Fix it.}

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
        \multicolumn{2}{c}{ALA} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} \\
	% ===
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
        \multicolumn{2}{c|}{TB} &
        \multicolumn{2}{c}{Current} \\
	% ===
	\rule{30pt}{0pt} &
        \multicolumn{2}{c|}{} &
	\multicolumn{1}{c|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{c|}{\rule{30pt}{0pt}} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} \\
	% ===
\cline{1-5}\cdashline{6-7}
\multicolumn{4}{|c|}{accepted lexeme} &
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working lexeme} \\
\cline{1-5}\cdashline{6-7}
        \rule{30pt}{0pt} &
        \multicolumn{2}{|p{6em}|}{\centering committed token} &
        \multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working token}
        \\
	% ===
\cline{1-5}\cdashline{6-7}
\multicolumn{5}{|c|}{seen input} &
        \multicolumn{2}{|c}{unseen input} \\
\cline{1-5}\cdashline{6-7}
\end{tabular}
\vspace{6pt}
\caption{Input structure with token split between lexemes}
\label{fig:input-3}
\end{figure}

\FloatBarrier

If \V{off} is negative,
as in Figure~\ref{fig:input-3} on page~\pageref{fig:input-3},
so that $\V{ALA} > \V{TB}$,
then
\begin{equation}
\label{eq:negative-offset}
	(\V{lex}, \V{off}) = \Velement{lex}{0, \Vsize{node}+\V{off}}.
\end{equation}

If \V{off} is negative, so that $\V{ALA} > \V{TB}$,
the beginning of the lexeme overlaps the last bytes
of \VFA{toString}{node},
so that
\begin{equation}
\label{eq:negative-overlap}
	\FA{overlap}{\V{node},\V{off}} =
		\element{(\VFA{toString}{node})}{\Vsize{node}+\V{off}, \Vsize{node}}.
\end{equation}

\section{[Material to be moved or deleted]}

\todo{Most of the material in this section should be moved to other places.}
Let \V{TClexes} be the set of token-consistent lexemes at
the ``click point'' in
Figure \ref{fig:try-push-byte} on page~\pageref{fig:try-push-byte}.
The test \tpbb will be true only if
\begin{equation}
	\label{eq:click-point-test}
	\exists \; \V{lex} \in \V{TClexes} \; \mid \; \FA{overlap}{\V{node},\V{off}} \, \sim \, (\V{lex}, \V{off}),
\end{equation}
where $\V{s} \, \sim \, \V{recce}$ is true iff the string \V{s} matches the recognizer
\V{recce}.
We say \tpbb is true ``only if'' \eqref{eq:click-point-test},
instead of ``if and only if'' \eqref{eq:click-point-test},
in order to allow for
``false positives''.

When \eqref{eq:click-point-test} is false,
the call to \tpb can be skipped.
For a tough lexeme \V{tuff}, the recognizer $(\V{tuff}, \V{off})$
might overapproximate, so that an unnecessary call
to \tpb might be made.
In practice,
allowed and token-consistent lexemes are sparse
most lexemes are sliceable,
and most tough lexemes have good overapproximations.

\needspace{3in}
\section{Caching}
\label{sec:caching}

For every node \V{node},
every lexeme \V{lex},
and every offset \V{off},
our optimization code must access the value of the GO-bit and
the EOL-bit.
We call
\begin{equation}
\label{eq:ruby-triple}
	(\V{node}, \V{lex}, \V{off})
\end{equation}
a ``Ruby triple'',

\begin{equation}
\label{eq:ruby-triple-test}
	\VFA{overlap}{\V{node},\V{off}} \, \sim \, (\V{lex}, \V{off}).
\end{equation}
To really achieve optimization,
the  results of \eqref{eq:ruby-triple-test}
for \eqref{eq:ruby-triple}
should be cached.
This is possible when \V{lex} is sliceable,
because then the value of \eqref{eq:ruby-triple-test}
depends only on the values of
\V{node}, \V{lex}, and \V{off}
so that
\begin{equation}
\label{eq:ruby-function}
	\FA{Ruby}{\V{node}, \V{lex}, \V{off}}
\end{equation}
is a boolean function.
We can make \eqref{eq:ruby-function}
a total function by using the wildcard slice
when \V{lex} is tough.

\todo{Add section about computation of EOL-bit}

\section{Fixed string example}
\label{sec:examples}

The example in the section will follow the recognition
of a fixed string lexeme, byte by byte.
The lexeme will be \texttt{/abcdef/}.

We will assume that we have already committed
a token which covers its first three bytes:
\texttt{"abc"},
so that our trie base (TB)
is 3 bytes after the
allowed lexeme anchor (ALA).
In other words, our lexeme offset is 3:
\begin{equation}
        \label{eq:fixed-string-offset}
	\V{off} = (\V{TB} \subtract \V{ALA}) = 3.
\end{equation}

We are trying to sample a token which covers
the last three bytes:
\texttt{"def"}.
We will show our progress until the EOL,
byte by byte.

\needspace{3in}
\FloatBarrier

\subsection {Byte 1: not at EOL}
{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & ...\texttt{"abc"} & \texttt{"d"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/abcdef/}} \\
		\hline
		LI Slice & & \texttt{/d/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Fixed string example}
\label{fig:example-fixed1}
\end{figure}
} % group for floatstyle

Figure~\ref{fig:example-fixed1} shows the situation where
\VFA{toString}{node},
the string implied by \V{node} of the token trie,
is \texttt{"d"}.
We are considering
the token-consistent lexeme \texttt{/abcdef/}.
The relevant range is \texttt{[3,4]},
and there is an LI slice: \texttt{/d/}.
Recall from \ref{eq:fixed-string-offset} that the lexeme offset is 3,
which is positive.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
\
\begin{equation}
\begin{gathered}
(\V{lex}, \V{off}) \\
	= \Velement{lex}{\V{off}, (\Vsize{node}+\V{off}) } \\
	= \Velement{(\texttt{/abcdef/})}{3, (1+3) } \\
    = \texttt{/d/}.
\end{gathered}
\end{equation}

Again, \VFA{toString}{node},
the string implied by \V{node} of the token trie,
is \texttt{"d"}.
Applying the formula for the overlap of positive offsets~\eqref{eq:positive-overlap},
\[
	\FA{overlap}{\V{node},\V{off}} = \VFA{toString}{node} = \texttt{"d"}.
\]
Attempting to match the overlap to the slice, we have
\[
     \texttt{"d"} \sim \texttt{/d/},
\]
which is a match,
so the GO-bit of our boolean pair is true.

The EOL-bit is false.
\todo{Explain this.}

So we cache $(\V{true},\V{false})$
for the Ruby triple
\[
	(\V{node}, \texttt{/abcdef/}, 3).
\]
The

\needspace{3in}
\FloatBarrier

\subsection {Byte 2: not at EOL}
{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & ...\texttt{"abc"} & \texttt{"de"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/abcdef/}} \\
		\hline
		LI Slice & & \texttt{/de/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Fixed string example}
\label{fig:example-fixed2}
\end{figure}
} % group for floatstyle

\needspace{3in}
\FloatBarrier

\subsection {Byte 3: EOL}
{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & ...\texttt{"abc"} & \texttt{"def"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/abcdef/}} \\
		\hline
		LI Slice & & \texttt{/def/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Fixed string example}
\label{fig:example-fixed3}
\end{figure}
} % group for floatstyle

Figure~\ref{fig:example-fixed3} shows the situation if
we have committed a token ending in \texttt{"abc"},
and \VFA{toString}{node},
the string implied by \V{node} of the token trie,
is \texttt{"def"}.
We are considering
the token-consistent lexeme \texttt{/abcdef/}.
The relevant range is \texttt{[3,6]},
and there is an LI slice: \texttt{/def/}.
The lexeme offset is
\[
	\V{off} = (\V{TB} \subtract \V{ALA}) = 3,
\]
which is positive.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
\
\begin{equation}
\begin{gathered}
(\V{lex}, \V{off}) \\
	= \Velement{lex}{\V{off}, (\Vsize{node}+\V{off}) } \\
	= \Velement{(\texttt{/abcdef/})}{3, (3+3) } \\
    = \texttt{/def/}.
\end{gathered}
\end{equation}

Again, \VFA{toString}{node},
the string implied by \V{node} of the token trie,
is \texttt{"def"}.
Applying the formula for the overlap of positive offsets~\eqref{eq:positive-overlap},
\[
	\FA{overlap}{\V{node},\V{off}} = \VFA{toString}{node} = \texttt{"def"}.
\]
Attempting to match the overlap to the slice, we have
\[
     \texttt{"def"} \sim \texttt{/def/},
\]
which is a match,
so we would cache a boolean true
for the Ruby triple
\[
	(\V{node}, \texttt{/abcdef/}, 3).
\]

\needspace{3in}
\FloatBarrier
\section {Kleene star example}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & ...\texttt{"aaa"} & \texttt{"aaa"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/aa*/}} \\
		\hline
		LI Slice & & \texttt{/aaa/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Kleene regex star example}
\label{fig:example-kleene}
\end{figure}
} % group for floatstyle

Figure~\ref{fig:example-kleene} illustrates the handling of Kleene star
regular expressions.
In Figure~\ref{fig:example-kleene}
we have committed a token ending in \texttt{"aaa"},
and \VFA{toString}{node}
is also \texttt{"aaa"}.
We are considering
the token-consistent lexeme \texttt{/aa*/}.
The relevant range is \texttt{[3,6]}
and there is an LI slice: \texttt{/aaa/}.\footnote{%
	For Kleene star regular expressions there will not necessarily
	be an LI slice.
	In that case we use overapproximation.
	See Section~\ref{sec:example-overapproximation}
	on page~\pageref{sec:example-overapproximation}
	for an
	example of overapproximation, in that case for regular
	expressions using alternatives.
}
The lexeme offset is
\[
	\V{off} = (\V{TB} \subtract \V{ALA}) = 3,
\]
which is positive.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
\
\begin{equation}
\begin{gathered}
(\V{lex}, \V{off}) \\
	= \Velement{lex}{\V{off}, (\Vsize{node}+\V{off}) } \\
	= \Velement{(\texttt{/aa*/})}{3, (3+3) } \\
    = \texttt{/aaa/}.
\end{gathered}
\end{equation}

Again, \VFA{toString}{node}
is \texttt{"aaa"}.
Applying the formula for the overlap of positive offsets~\eqref{eq:positive-overlap},
\[
	\FA{overlap}{\V{node},\V{off}} = \VFA{toString}{node} = \texttt{"aaa"}.
\]
and matching the overlap against the slice,
\begin{equation}
\label{eq:aaa-match}
     \texttt{"aaa"} \sim \texttt{/aaa/}.
\end{equation}
\eqref{eq:aaa-match} is a match,
so we would cache a boolean true
for the Ruby triple
$(\V{node}, \texttt{/aa*/}, 3)$.

\needspace{3in}
\FloatBarrier
\section {Examples without LI slice}
\label{sec:example-no-li-slice}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[H]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & \texttt{..."aaa"} & \texttt{"bbb"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/(aaaaaa|bbbbbb)/}} \\
		\hline
		LI Slice	& & none \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{No LI slice example}
\label{fig:example-no-li-slice}
\end{figure}
} % group for floatstyle

In the case of
Figure~\ref{fig:example-no-li-slice},
we are considering the lexeme
\texttt{/(aaaaaa|bbbbbb)/} for which
there is no LI slice.
We must \mbox{remedy} this in order
to determine a boolean true
for the Ruby triple
$(\V{node}, \texttt{/(aaaaaa|bbbbbb)/}, 3)$.
Section~\ref{sec:example-factored}
on page~\pageref{fig:example-factored}
shows a solution that involves factoring
\texttt{/(aaaaaa|bbbbbb)/}
into two lexemes,
one for each alternative.
Section~\ref{sec:example-overapproximation}
on page~\pageref{fig:example-overapproximation}
shows a solution that uses
overapproximation.

\needspace{3in}
\FloatBarrier
\subsection{Example of factored alternatives}
\label{sec:example-factored}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & \texttt{..."aaa"} & \texttt{"aaa"} \\
		\hline
		\multirow{2}{*}{Lexemes}
			& \multicolumn{2}{|c|}{\texttt{/aaaaaa/}} \\
			 & \multicolumn{2}{|c|}{\texttt{/bbbbbb/}} \\
		\hline
		\multirow{2}{*}{LI Slices} & & \texttt{/aaa/} \\
		 & & \texttt{/bbb/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Factored regular expression example}
\label{fig:example-factored}
\end{figure}
} % group for floatstyle

One solution to the problem presented in
Section~\ref{sec:example-no-li-slice}
on page~\pageref{sec:example-no-li-slice}
is ``factoring''.
Any regular expression which does not use the Kleene star
can be factored into fixed string lexemes.
We do note that such regular expressions might still be tough:
the number of factored strings can be large enough to make
a factoring solution impractical.

In this example, factoring is quite practical.
\texttt{/(aaaaaa|bbbbbb)/}
can be factored into the two lexemes
\texttt{/aaaaaa/} and \texttt{/bbbbbb/}.
As in the previous examples, the offset is 3,
and the range is \texttt{[3,6]}.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
their slices are easily found to be, respectively,
\texttt{/aaa/} and \texttt{/bbb/}.
\VFA{toString}{node} is \texttt{"aaa"},
and \FA{overlap}{\V{node},3} is \texttt{"aaa"}.
Therefore
\texttt{/aaa/} matches
and \texttt{/bbb/} does not.
We cache a boolean true
for the Ruby triple
$(\V{node}, \texttt{/aaaaaa/}, 3)$
and a boolean false
for the Ruby triple
$(\V{node}, \texttt{/bbbbbb/}, 3)$.

\subsection {Example of overapproximation}
\label{sec:example-overapproximation}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[H]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Token segments & ...\texttt{"aaa"} & \texttt{"abc"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/(aaaaaa|bbbbbb)/}} \\
		\hline
		Overapproximated LI Slice & & \texttt{/[ab][ab][ab]/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Overapproximated slice example}
\label{fig:example-overapproximation}
\end{figure}
} % group for floatstyle

A second solution to the problem presented in
Section~\ref{sec:example-no-li-slice}
on page~\pageref{sec:example-no-li-slice}
is overapproximation.
Again, our lexeme is
\texttt{/(aaaaaa|bbbbbb)/}.
Again, the offset is 3,
and the range is \texttt{[3,6]}.
An excellent overapproximated slice for
\element{(\texttt{/(aaaaaa|bbbbbb)/})}{3,6} is
\texttt{/[ab][ab][ab]/}.
\VFA{toString}{node} is \texttt{"abc"},
which means that
\FA{overlap}{\V{node},3} is \texttt{"abc"},
which does not match.
We therefore cache a boolean false
for the Ruby triple
$(\V{node}, \texttt{/(aaaaaa|bbbbbb)/}, 3)$.

We note that,
if \VFA{toString}{node} was \texttt{"aaa"},
we would match,
but our match would be a false positive.

\FloatBarrier

\section {Example of negative offset}
\label{sec:negative-offset-example}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[H]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[TB,ALA]}
			& \multicolumn{1}{|c|}{[ALA,Current]}
			\\
		\hline
		Token segments & ...\texttt{"xyz"} & \texttt{"abb"} \\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/ab*/}} \\
		\hline
		LI Slice & & \texttt{/abb/} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Negative offset example}
\label{fig:example-negative-offset}
\end{figure}
} % group for floatstyle

All our examples so far have been positive-offset lexemes.
That is because they present the most difficult cases.
In this section we look at a negative-offset lexeme

Our lexeme is \texttt{/ab*/}.
The lexeme offset is
\[
	\V{off} = (\V{TB} \subtract \V{ALA}) = {-3},
\]
which is negative.
Applying the formula for slices of negative offsets~\eqref{eq:negative-offset},
\
\begin{equation}
\begin{gathered}
(\V{lex}, \V{off}) \\
	= \Velement{lex}{0, \Vsize{node}+\V{off}} \\
	= \Velement{(\texttt{/ab*/})}{0, 6+(-3)} \\
	= \Velement{(\texttt{/ab*/})}{0, 3} \\
    = \texttt{/abb/}.
\end{gathered}
\end{equation}
We note that all slices of the lexemes for
negative offsets start at 0,
so that
regular expression lexemes with negative offsets
are always sliceable.

Determining the overlap for a negative offset is slightly more
complicated that it has been for the positive values of \V{off}.
Applying the formula for overlaps of negative offsets,
~\eqref{eq:negative-overlap}
on page ~\pageref{eq:negative-overlap},
\begin{equation}
\begin{gathered}
	\FA{overlap}{\V{node},\V{off}} \\
	= \element{(\VFA{toString}{node})}{\Vsize{node}+\V{off}, \Vsize{node}}.\\
	= \element{(\texttt{"xyzabc"})}{6+(-3), 6} \\
	= \element{(\texttt{"xyzabc"})}{3, 6} \\
	= \texttt{"abc"}.
\end{gathered}
\end{equation}

Comparing the overlap to the slice,
\[
     \texttt{"abc"} \sim \texttt{/abb/}
\]
is not a match.
We therefore cache a boolean false
for the Ruby triple
\[
	(\V{node}, \texttt{/ab*/}, {-3}).
\]

\FloatBarrier

\section{Caching strategy}
\label{sec:caching-strategy}

It may be best to vary the caching strategy.
For small absolute values of \V{off}, the cache
will be dense,
and the likelihood of hits in a fully pre-populated
cache will be high.
In this case,
pre-computation at grammar compile time makes sense.
Computing many of these values in a single token trie
traversal would allow for optimizations.

\begin{MYsloppy}
Lexemes may be of arbitrary length,
so that, in practice,
pre-computation of all Ruby triple values
is not just costly, but impossible.
Further,
in practical applications,
the lexemes of
very large absolute values of \V{off}
will usually be very long strings or here-documents.
Long strings and here-documents
are unlikely to be repeated exactly,
so that the likelihood of cache hits would
be very low.
Therefore, for absolute values of \V{off}
above a certain threshold,
it is likely to make sense to not cache the
value of Ruby triples.
\end{MYsloppy}

So far in this section
we have discussed varying the caching strategy
only for different absolute values of \V{off}.
Further investigation
may show that it makes sense to
take into account the sign of \V{off},
or to vary the caching strategy by
lexeme.

\section{Stage 2: Track ends of lexeme}
\label{sec:stage2}

We call a location where a lexeme can end,
an ``end of lexeme'', or EOL.
In Stage 2, we build on Stage 1 by tracking EOLs.
In cases where \tpbb
is called when we are not at an EOL,
\tpbb simply stores byte \V{b}.
If we skip the call to \tpbb,
but turn out to need the value of byte \V{b}
later, we can find it from context.
Even more than Stage 1,
Stage 2 is likely to require modifications
to \tpb.

To implement Stage 2,
we will want to cache a boolean that indicates
whether every Ruby triple is at an end of lexeme.
\begin{equation}
\label{eq:eol-function}
	\FA{EOL}{\V{node}, \V{lex}, \V{off}}
\end{equation}
We need not cache values of
\eqref{eq:eol-function} when
the \Fname{Ruby} function~\eqref{eq:ruby-function}
is false.
Also,
whenever 
the \Fname{Ruby} function~\eqref{eq:ruby-function}
is true due to an overapproximation,
\eqref{eq:eol-function} should be set to true.

In Stage 2, calls to \tpbb
need only be made at end of lexeme,
and for tough lexemes.
As noted previously,
token-consistent lexemes in general,
and tough lexemes in particular,
should be sparse,
and calls to \tpbb
should be reduced greatly.

\section{Forcing}

The optimization described here detects all situations where there is
only one token-consistent lexeme at a location.
In this way, it can be said to auto-detect all cases of forcing.
This is done as a natural extension of its usual logic for
token-consistent lexemes and their tokenization,
rather than by special-casing.

\FloatBarrier

{
\clearpage

% Ragged right, do not hyphenate.
\RaggedRight
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Silence current hbox warnings, but allow them if
% badness increases further
\hbadness=2000

\begin{thebibliography}{10}

\bibitem{Kegler2011a}
\newblock{Jeffrey Kegler.}
\newblock{``What is the Marpa algorithm?''.}
\newblock{\url{https://blogs.perl.org/users/jeffrey_kegler/2011/11/what-is-the-marpa-algorithm.html}.
   Retrieved 2 December, 2024.}

\bibitem{Kegler2011b}
\newblock{Jeffrey Kegler.}
\newblock{``Marpa and the Ruby Slippers''.}
\newblock{\url{http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2011/11/marpa-and-the-ruby-slippers.html}.
   Retrieved 2 December, 2024.}

\bibitem{Kegler2023}
\newblock{Jeffrey Kegler.}
\newblock{``Marpa, A practical general parser: the recognizer''.}
\newblock{\url{https://arxiv.org/abs/1910.08129}.
   Retrieved 2 December, 2024.}

\bibitem{llguidance}
\newblock{llguidance team.}
\newblock{"Low-level guidance parser".}
\newblock{
  \url{https://github.com/microsoft/llguidance}.
  Retrieved 2 December, 2024.}

\bibitem{wiki-loop-invariant}
\newblock{Wikipedia contributors.}
\newblock{"Loop-invariant code motion --- {Wikipedia}{,} The Free Encyclopedia".}
\newblock{\url{https://en.wikipedia.org/w/index.php?title=Loop-invariant_code_motion&oldid=1249559170}.
	Retrieved 16 December 2024.}

\bibitem{XGrammar2024}
\newblock{Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao and Tianqi Chen.}
\newblock{``XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models''.}
\newblock{\url{https://arxiv.org/abs/2411.15100}.
	Retrieved 16 December 2024.}

\end{thebibliography}

} % RaggedRight

\clearpage
\tableofcontents

% \clearpage
% \phantomsection
% \listofalgorithms

\ifdraft{
    \clearpage
    \phantomsection
    \listoftodos
}{}



\end{document}
