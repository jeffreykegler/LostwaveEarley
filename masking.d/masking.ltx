\documentclass[draft,12pt]{amsart}

\usepackage[final]{listings}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{needspace}

\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{ragged2e}
\usepackage{url}
\usepackage[final]{hyperref}
\usepackage{placeins}
\usepackage{mfirstuc}
\usepackage{ifdraft}
\usepackage[obeyDraft]{todonotes}
\usepackage{datetime2}
\ifdraft{
    \usepackage{draftwatermark}
}{}

% Fix conflict between todonotes and amsart packages
% See http://ctan.math.washington.edu/tex-archive/macros/latex/contrib/todonotes/todonotes.pdf,
% p. 10.
\makeatletter
\providecommand\@dotsep{5}
\def\listtodoname{List of TODOs}
\def\listoftodos{\@starttoc{tdo}\listtodoname}
\makeatother

\makeatletter
\renewcommand{\listofalgorithms}{\@starttoc{loa}{List of algorithms}}
\let\l@algorithm=\l@figure
\makeatother

% Re hbox's, see
% https://tex.stackexchange.com/questions/241343/what-is-the-meaning-of-fussy-sloppy-emergencystretch-tolerance-hbadness
\raggedbottom

% Replaced by todonotes package
% \newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}

\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}
\newcommand{\mymathbin}[1]{\mathbin{\texttt{#1}}}

% For a type name, when it occurs in text
\newcommand{\type}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\mult}{\mathbin{\ast}}
\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\bigsize}[1]{\ensuremath{\bigl| {#1} \bigr|}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}\xspace}
\newcommand{\Oc}{\order{1}}
\newcommand{\On}{\order{\var{n}}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}

\newcommand{\lastix}[1]{\ensuremath{\##1}}
\newcommand{\Vlastix}[1]{\ensuremath{\lastix{\V{#1}}}}
\newcommand{\VlastElement}[1]{\Velement{#1}{\Vlastix{#1}}}
\newcommand{\element}[2]{\ensuremath{\mathop{#1}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\Velement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\VVelement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[ \V{#2}
    \right]\mathclose{} }}

\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\mcolon}{\mathrel:}
\newcommand{\mcoloncolon}{\mathrel{\vcenter{\hbox{$::$}}}}

\newcommand{\suchthat}{\mcoloncolon}
\newcommand{\quantify}[3]{% quantifier, binding, predicate
    \ensuremath{\mathrel{#1}#2 \; \suchthat \; #3}%
}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\V}[1]{\ensuremath{\texttt{\mbox{#1}}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\Rightarrow}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\:\stackrel{\!{+}}{\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

% For the type subscript for powersets
\newcommand{\setOf}[1]{{{#1}^{\mbox{\normalsize $\ast$}}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\bigset}[1]{{\bigl\lbrace #1 \bigr\rbrace} }
\newcommand{\Bigset}[1]{{\Bigl\lbrace #1 \Bigr\rbrace} }

% use internal counter of algorithmicx
\newcommand{\algnested}{ %
  \makeatletter\ALG@nested\makeatother %
}

\newcommand{\algstrut}{\rule{0pt}{.85\baselineskip}}

\newcommand{\myparbox}[2][\algparwidth]{%
  \parbox[t]{#1}{%
  % \linespread{1.15}\selectfont
  \ignorespaces#2\par%
  }%
}

\newcommand{\parcomment}[2]{\hspace{\dimexpr \algorithmicindent * #1}$\triangleright$ #2}

% \newcommand{\rawparcomment}[2][\algparwidth]{%
  % \myparbox[#1]{#2}%
% }
% \newcommand{\parcomment}[2][\algparwidth]{%
  % \myparbox[#1]{$\triangleright$ #2}%
% }
% \newcommand{\parcomment}[1]{\hspace{\dimexpr \algorithmicindent - \labelwidth }$\triangleright$
  % \protect\parbox[t]{\dimexpr \textwidth - (\algorithmicindent - \labelwidth)}{#1}%
% }

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newcommand{\mname}[1]{\mbox{\sf #1}}
% [O]perator [A]pplication
\newcommand{\Oname}[1]{\mname{#1}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\OA}[2]{\ensuremath{%
    \mathop{\Oname{#1}}\mathopen{}\left(#2\right)\mathclose{}%
}}
\newcommand{\VOA}[2]{\OA{#1}{\V{#2}}}
% [F]unction [A]pplication, replaces \myfn
\newcommand{\smallrawfn}[2]{\ensuremath{\mathop{#1}(#2)}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\rawfn}[2]{\ensuremath{\mathop{#1}\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\Fname}[1]{\V{#1}}
\newcommand{\FA}[2]{\ensuremath{\rawfn{\mathop{\Fname{#1}}}{#2}}}
\newcommand{\VFA}[2]{\FA{#1}{\V{#2}}}

\hyphenation{oper-and oper-ands}
\hyphenation{look-ahead}
\hyphenation{memo-ization}

% I like to silence underfull hbox messages.
% This is syntactic sugar for doing that.
\newenvironment{MYsloppy}[1][3em]{\par\tolerance9999 \emergencystretch#1\relax}{\par}
% This form allows the resetting of hbadness, which may be necessary to
% shut up an "underfull" warning.
\newenvironment{MYsloppyB}[2]{\par\tolerance9999 \emergencystretch#1 \hbadness#2\relax}{\par}
% This form is for a trick:  Tolerances are evaluated line by line, so a tolerance
% less than 9999 may produce a better looking paragraph and reduce the "badness".
\newenvironment{MYsloppyC}[3]{\par\tolerance#3 \emergencystretch#1 \hbadness#2\relax}{\par}

\title
  [Masking Thoughts]
  {Thoughts on optimizing the computation of masks}

\author{Jeffrey Kegler}
\ifdraft{
    % legacy draftwatermark options -- update?
    \SetWatermarkLightness{0}
    \SetWatermarkText{DRAFT \DTMnow{} DRAFT}
    \SetWatermarkAngle{0}
    % \SetWatermarkScale{1}
    % \SetWatermarkHorCenter{1.5in}
    \SetWatermarkVerCenter{.5in}
}{}
\ifdraft{ \thanks{DRAFT as of \DTMnow} }{}
\date{\DTMnow.}

\newcommand{\tpb}{\url{try\_push\_byte}\xspace}
\newcommand{\tpbb}{\url{try\_push\_byte(b)}\xspace}
\newcommand{\al}{\url{allowed\_lexemes}\xspace}
\newcommand{\llguidance}{{\tt llguidance}\xspace}

\begin{document}

\maketitle
\tableofcontents

\section{About this document}
\label{sec:about-me}

This document outlines a method for optimizing mask computation in
Guidance.
It assumes some familiarity with the code in
\llguidance\cite{llguidance}.

\section{Conventions}
\begin{itemize}
\item ``Iff'' (or ``iff'') abbreviates
	``if and only if''.
\item Ranges are inclusive start,
	exclusive end so that, for example,
		\begin{gather*}
			\element{(\texttt{"abcd"})}{0,2} = \texttt{"ab"} \text{, and} \\
			\element{(\texttt{"abcd"})}{2,3} = \texttt{"c"}.
		\end{gather*}
\end{itemize}

\section{The first basic idea}

The first basic idea behind the suggestions in this document is
the well-known one of taking invariant code called repeatedly,
arranging for it to be called only once.\cite{wiki-loop-invariant}
In our case, instead of moving the invariant code before the repeating
logic,
we may compute its value once and cache it.\footnote{%
	See Section \ref{sec:caching}
	on page~\pageref{sec:caching}.}

To be ``invariant'' in a sense useful for us,
the result logic must be ``location-independent'' (LI),
in the sense that it produces the same result
regardless of the parse location at which it is called.
Logic which is not location-independent is
called ``location-dependent''.

\section{The second basic idea}

The second basic idea behind this document is more
aggressive use of the Ruby Slippers.
The Ruby Slippers is
a parsing technique discovered independently by Jeffrey Kegler%
\cite{Kegler2011a}\cite{Kegler2011b}\cite{Kegler2023}
and
Michal Moskal\cite{llguidance}.
It takes advantage of the ability of certain variants of the Earley
parser to tell the application
exactly which terminals will be acceptable at
any point in the parse.
This allows the application to change the lexeme stream to match
the parser's expectations.
The application could, for example, invent a lexeme on the fly
to accomodate the parser.\footnote{%
    The name ``Ruby Slippers'' comes from a incident in the movie
    {\it Wizard of Oz}.  In it Dorothy, the protagonist, is trying
    to return to her home in Kansas.  After many adventures she
    discovers that a pair of ruby slippers, which she has been
    wearing throughout the movie, grant her this ability.
    She has merely to click her heels and wish.}

The Ruby Slippers are already in use
in \llguidance.
An \al vector is computed,
and applied at certain points.

Looking ahead
in this document, we are going to claim
that the Ruby Slippers can be used earlier and more extensively,
and that most of the work of applying the
Ruby Slippers is location-independent.
It is hoped that this will allow considerable savings in time.
\todo{use XGrammar's ``1\%'' statistic (section 3.1)?}

Need to access information about the stack is often
send as an obstacle to location-independence:
Any computation which refers to the stack, it is thought,
must be location dependent.\footnote{%
	For example,
	from Section 1, ``Introduction'' in \cite{XGrammar2024}:
	``CFG interpretation
	requires a stack state that tracks the recursive rules matched
	so far, making it impossible to precompute and cache all
	combinatorial combinations of stack patterns ahead of time.''}
We can use the Ruby Slippers, however, to make
most computation involving the stack location-independent.

To do this, we note that the only stack information we care
about is the list of ``allowed lexemes'', and information
about them.
The information about the allowed lexemes is location-independent,
and, in a Ruby-Slippers-ready implementation of the Earley algorithm,
like \llguidance,
the list of allowed lexemes is quickly found.
In fact, in \llguidance, the list of allowed lexemes
is already computed as \al,
so that it comes at literally zero cost.
For our purposes, the only cause of location-dependence will
be reference to portions of the input too early
to be found in the token trie.

The results of the LI logic are single booleans,
which are cached and used to avoid
calls to \tpbb.
Using this optimization,
\tpbb typically will only be called
when a new lexeme is ready to be accepted.
In a minority of instances,
the logic necessary to determine when
a new lexeme is ready to be accepted
cannot be made LI.
In those cases,
the cached booleans will be overapproximations,
and \tpbb is used as a fallback.

\FloatBarrier

\needspace{3in}

\section{The structure of the byte stream}
\label{sec:byte-stream}

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
	\multicolumn{2}{c}{TB} &
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\rule{30pt}{0pt} &
	\multicolumn{2}{c|}{ALA} &
\multicolumn{2}{p{72pt}}{\centering Current} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
	\multicolumn{2}{p{72pt}|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{p{30pt}|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{p{30pt}|}{\rule{30pt}{0pt}} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-5}
	\multicolumn{2}{|p{72pt}|}{\centering \hyphenpenalty=10000 accepted lexeme} &
	% 30pt * 5 + 12pt * 4 = 198pt
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt} working lexeme}
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-6}
\rule{30pt}{0pt} &
	\multicolumn{2}{|p{72pt}|}{\centering committed token} &
	% 30pt * 3 + 12pt * 2 = 114pt
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt} working token} &
        \rule{30pt}{0pt} \\
	% ===
\cline{1-4}\cdashline{5-7}
	\multicolumn{4}{|p{156pt}|}{\centering seen input} &
	\multicolumn{3}{|p{114pt}}{\centering unseen input} \\
\cline{1-4}\cdashline{5-7}
\end{tabular}
\par\vspace{12pt}
\noindent
``Current'' is Current location in the byte stream. \\
``ALA'' is the Allowed Lexeme Anchor. \\
``TB'' is the Toktrie Base. \\
\vspace{6pt}
\caption{Input structure}
\label{fig:input-1}
\end{figure}

\FloatBarrier

We assuming some knowledge of the codebase of \llguidance.
As a reminder, in performing the mask computation we simulate byte streams
which are divided into tokens,
and check them for acceptability against a parser which divides that same byte stream into lexemes.
From the parser
we are given a list of acceptable lexemes.
We want to use this to determine which tokens are consistent with the parse so far.
Once we know which tokens are parse-consistent,
we can produce a token mask,
which in turn will allow us to sample a token.

To determine the acceptable tokens, we travserse a trie of the tokens,
implicitly checking the tokens against all possible byte streams.
Every node in the trie traversal implies a single byte stream,
which may be seen as structured according to the scheme in
Figure \ref{fig:input-1}
on page~\pageref{fig:input-1}.

In Figure \ref{fig:input-1}
and our other
byte stream diagrams, we have identified
\begin{itemize}
	\item the most recently accepted lexeme,
	\item the most recently committed token,
	\item a lexeme in progress (aka the ''working lexeme''), and
	\item a token in progress (aka the ''working token'').
\end{itemize}
In the byte stream, we note three locations of special significance:
\begin{itemize}
	\item The current location, which is the location in the input stream
		corresponding to our current position in the our token trie traversal.
		We abbreviate this as \V{Current}.
	\item The ``toktrie base'', that is, the start location for our toktrie
		traversal, which we abbreviate \V{TB}.
		\V{TB} is at the end of the last committed token,
		if it exists, and at the start of parsing otherwise.
	\item The location for which we calculated the allowed lexemes,
		that is, the ``allowed lexeme location''.
		We abbreviate this as \V{ALA}.
		\V{ALA} is at the end of the last accepted lexeme,
		if it exists, and at the start of parsing otherwise.
\end{itemize}

It is always the case the current location is at or after TB:
\begin{equation}
\label{eq:invariant-1}
\V{TB} \le \V{Current}.
\end{equation}
It is also always the case the current location
is at or after the ALA:
\begin{equation}
\label{eq:invariant-2}
\V{ALA} \le \V{Current}.
\end{equation}

Our byte stream diagrams will make some assumptions
without loss of generality.
We will assume
there is a last committed token and
a last accepted lexeme.
And, while the invariants of
equation \ref{eq:invariant-1} on page~\pageref{eq:invariant-1} and
equation \ref{eq:invariant-2} on page~\pageref{eq:invariant-2}
will always be obeyed,
otherwise our byte stream diagrams may make arbitrary
assumptions about the relative length of the tokens and lexemes.
For example, Figure \ref{fig:input-2}
on page~\pageref{fig:input-2}
is similar to Figure \ref{fig:input-1}
on page~\pageref{fig:input-1},
except that, while Figure \ref{fig:input-1} shows a working
lexeme that ends {\bf before} the working token,
Figure \ref{fig:input-1} shows a working
token that ends {\bf after} the working token.

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
\multicolumn{2}{c}{TB} &
\rule{30pt}{0pt} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\rule{30pt}{0pt} &
\multicolumn{2}{c|}{ALA} &
\multicolumn{2}{c}{Current} &
\rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\multicolumn{2}{c|}{} &
\multicolumn{1}{c|}{} &
\multicolumn{1}{c|}{} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} &
        \rule{30pt}{0pt} \\
\cline{1-4}\cdashline{5-7}
	\multicolumn{2}{|p{72pt}|}{\centering \hyphenpenalty=10000 accepted lexeme} &
	% 30pt * 5 + 12pt * 4 = 198pt
	\multicolumn{5}{|p{198pt};{2pt/2pt}}{\centering\vspace{-1pt}working lexeme} \\
\cline{1-4}\cdashline{5-7}
\rule{30pt}{0pt} &
	\multicolumn{2}{|p{72pt}|}{\centering committed token} &
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working token} &
        \rule{30pt}{0pt} \\
\cline{1-4}\cdashline{5-7}
	\multicolumn{4}{|p{156pt}|}{\centering seen input} &
	\multicolumn{3}{|p{114pt}}{\centering unseen input} \\
\cline{1-4}\cdashline{5-7}
\end{tabular}
\vspace{6pt}
\caption{Input structure with long working lexeme}
\label{fig:input-2}
\end{figure}

\needspace{100pt}
The \V{ALA} may occur before or after the \V{TB}.
Where
\begin{equation}
\label{eq:lexeme-offset}
	\V{off} = \V{TB} \subtract \V{ALA},
\end{equation}
we call \V{off} the
{\bf lexeme offset}\label{loc:lexeme-offset}.

\section{Lexeme slices}

Lexeme slices are like string slices, and are represented similarly,
so that $\Velement{lex}{0,2}$ is a lexeme which matches the first two
characters of a string which matches \V{lex} and $\Velement{lex}{2,3}$
is a lexeme which matches the third character of a string which matches
\V{lex}.

In this document, we refer to a lexeme slice as location-independent (LI)
if the slice matches correctly regardless of any earlier bytes in the
string.
For example,
\MYsloppy{%
\begin{enumerate}
\item
    $\element{(\texttt{/abcd/})}{2,3} = \texttt{/c/}$
    is an LI slice, because the string \texttt{"c"}
    will always match the third character of any string
    which matches $\texttt{/abcd/}$.
\item
\label{li:unsliceable}
    $\element{(\texttt{/(aaaaaa|bbbbbb)/})}{2,3}$ does not have an LI slice,
    because the third character of it must be either
    \texttt{"a"} or \texttt{"b"}.
    If the third character
    is \texttt{"a"} it will not match the third character of
		\texttt{"bbbbbb"} even though
		\texttt{"bbbbbb"} matches
    \texttt{/(aaaaaa|bbbbbb)/}.
    If the third character
    is \texttt{"b"} it will not match the third character of
		\texttt{"aaaaaa"} even though
		\texttt{"aaaaaa"} matches
    \texttt{/(aaaaaa|bbbbbb)/}.
    The $\element{(\texttt{/(aaaaaa|bbbbbb)/})}{2,3}$
		can be dealt with by overapproximation
		(see Section \ref{sec:overapproximation}
		on page~\pageref{sec:overapproximation}).
\item
    $\element{(\texttt{/(aaaaaa|bbbbbb)/})}{0,3}
		= \texttt{/(aaa|bbb)/}$ is an LI slice.
		In fact, every slice which begins at location
		0 of the lexeme is an LI slice.
\item
\label{li:kleene}
    Kleene star regular expressions often have LI slices.
    $\element{(\texttt{/a*/})}{2,4}
		= \texttt{/aa/}$ is an LI slice.
\item
	Case-insensitiviy is not a problem for slicing.
	Writing case-insensitive \texttt{/lorem/} as
	\texttt{/[Ll][Oo][Rr][Ee][Mm]/},
		we see that
		$\element{(\texttt{/[Ll][Oo][Rr][Ee][Mm]/ })}{2,3} = \texttt{/[Rr]/}$
		is an LI slice.
\end{enumerate}
} % end of Mysloppy

If there is a range for which there is no LI slice
for a lexeme,
we say that it is ``unsliceable''.
When slicing a lexeme is problematic,
we say that the lexeme is ``tough''

We consider cases.

\begin{itemize}
	\item Every fixed string is sliceable,
and none of them are tough.
In practice, most
lexemes are fixed strings.
\item
When a lexeme has no Kleene star,
it is sliceable by ``factoring'' it into fixed
strings.
Some factorings may be exponential, however,
so that regular expression lexemes,
even would Kleene stars, can be tough.
\item
	Not all Kleene star regular expressions
	are sliceable, but many are,
	as we saw in the example numbered \ref{li:kleene}
	on page~\ref{li:kleene}.
\item Lexemes can be many other things besides regular expressions,
including subgrammars.
Some of these problematic lexemes may be sliceable nonetheless.
For example, the application could explicitly specify
how they can be sliced.
But many of these will be tough lexemes.
\end{itemize}

\section{Overapproximation}
\label{sec:overapproximation}

Above, we introduced the idea of tough and unsliceable lexemes.
These can be handled with overapproximation.
Our overapproximation must not reject any matching lexemes (``false negative''),
but they may accept non-matching lexemes (``false positives'').

As an example of overapproximation, we revisit
example \ref{li:unsliceable} on
page~\pageref{li:unsliceable}:
\begin{equation}
	\label{eq:unsliceable}
    \element{(\texttt{/(aaaaaa|bbbbbb)/})}{2,3}
\end{equation}
is not sliceable.
But a good overapproximation to
\eqref{eq:unsliceable} is
\begin{equation}
	\label{eq:approx}
\texttt{/[ab]/},
\end{equation}
which matches either
\texttt{"a"} or \texttt{"b"}.
\eqref{eq:approx} avoids false negatives,
as required.
\eqref{eq:approx} does return false positives,
but only in the case of one byte out of
the 256 possibilities,
so the accuracy of
\eqref{eq:approx} is high.

The overapproximation of last resort is a ``wildcard'',
which matches everything.
Almost all lexemes will be sliceable in practice,
and an implementation is likely to find it most convenient
to treat all lexemes as if they were sliceable.
The wildcard approximation allows this.

\section{Lexeme slices and token trie nodes}

In the context of token trie nodes,
lexeme slices anchored to either side of the toktrie base (TB)
are of special interest.
We call these ``anchored lexeme slices''.
Anchored lexeme slices can be conveniently represented as a duple.

Recall (equation \ref{eq:lexeme-offset} on page~\pageref{loc:lexeme-offset})
that we defined the lexeme offset as \V{off}
where
\[
	\V{off} = \V{TB} \subtract \V{ALA}.
\]
An anchored lexeme slice can be represented
as $(\V{lex}, \V{off})$.

For convenience in defining anchored lexeme slices
we first set out some definitions.
\begin{itemize}
	\item Let \V{node} be a token trie node.
	\item Let \VFA{path}{node} be the path from the trie root
to \V{node}, treated as a vector of nodes.
\item We represent the length of \VFA{path}{node}
as \size{\VFA{path}{node}}.
Abusing notation a bit, we also write
\size{\VFA{path}{node}} as \Vsize{node}.
\Vsize{node} is also called the ``depth''
of \V{node}.
\item The node vector \VFA{path}{node}, mapped to
a string of bytes, is \FA{toString}{\VFA{path}{node}}.
We usually overload the notation
and write \FA{toString}{\VFA{path}{node}}
as \VFA{toString}{node}.
\end{itemize}

With these definition, we are in a position to state the value
of the anchored lexeme slice duples.
If \V{off} is positive, so that $\V{TB} > \V{ALA}$,
\begin{equation}
\label{eq:positive-offset}
	(\V{lex}, \V{off}) = \Velement{lex}{\V{off}, \Vsize{node}+\V{off}}.
\end{equation}
If \V{off} is negative, so that $\V{ALA} > \V{TB}$,
then
\begin{equation}
\label{eq:negative-offset}
	(\V{lex}, \V{off}) = \Velement{lex}{0, \Vsize{node}}.
\end{equation}

\section{Token-consistent lexemes}

At the start of parse, and after every lexeme,
there is a set of ``allowed lexemes''.
The allowed lexemes are those which, if accepted,
can result in a valid parse,
according to the parser.
\llguidance already calculates the allowed lexemes,
which it tracks in its \al vector.

Recall the location for which the allowed lexemes
are calculated is called the ALA.
The location where the last committed token ends
is TB.
If $\V{TB} > \V{ALA}$, it becomes possible
that an allowed lexeme might be inconsistent
with the committed tokens.

For example, if the lexemes {\tt foreach} and {\tt while} are
allowed at location $\ell$,
and the token {\tt "wh"} is committed, starting at
$\ell$,
then only the lexeme {\tt while} will be consistent with
the committed tokens.
While according to the parser, the lexeme {\tt foreach}, if accepted,
could result in a valid parse,
the lexeme {\tt foreach} would be inconsistent with the
committed tokens
and would cause the parse to become inconsistent,
and/or fail.

We therefore, whenever we commit a token,
calculate the subset of the allowed lexemes
which can still produce a parse both in terms
of the parser and the tokenization.
We call these the ``token-consistent lexemes''.

The token-consistent lexemes might be equivalent
to the allowed lexemes.
When $\V{TB} \le \V{ALA}$, this will always be
the case.
A token-consistent lexeme is always an allowed lexeme.

\section{Stage 1: Calculating the allowed bytes}

We set out our algorithm in two stages.
While each stage does result in an optimization,
our separation of the algorithm into stages
is primarily to simplify the presentation.
It is not (at last not necessarily)
a suggestion for gradual implementation.

In Stage 1, we go from the token-consistent lexemes
to the allowed bytes of tokens,
and use this to eliminate calls to
\tpb.
We refer to the code snippet\footnote{%
    This code snippet is similar to, and taken from,
    \url{https://github.com/microsoft/llguidance/blob/0ca091a701a50134e0503fa03c5c12b206e182a3/toktrie/src/toktree.rs\#L945}.
}
in Figure \ref{fig:try-push-byte} on page~\pageref{fig:try-push-byte}.
\begin{figure}
\begin{lstlisting}
    while p < endp {
        r.pop_bytes(next_pop);
        let n = &self.nodes[p];
        let b = n.byte();
        // Click ruby slipper heels here
        if r.try_push_byte(b) {
            toks.allow_token(n.token_id().
                unwrap_or(defl_tok)
            );
            // ... stuff here ...
        } else {
            // ... other stuff here ...
        }
    }
\end{lstlisting}
\caption{Ruby Slippers Click Point%
    \label{fig:try-push-byte}%
}
\end{figure}

Let \V{TClexes} be the set of token-consistent lexemes at
the ``click point'' in
Figure \ref{fig:try-push-byte} on page~\pageref{fig:try-push-byte}.
The test \tpbb will be true only if
\begin{equation}
	\label{eq:click-point-test}
	\exists \; \V{lex} \in \V{TClexes} \; \mid \; \VFA{toString}{node} \, \sim \, (\V{lex}, \V{off}),
\end{equation}
where $\V{s} \, \sim \, \V{recce}$ is true iff the string \V{s} matches the recognizer
\V{recce}.
When \eqref{eq:click-point-test} is false,
the call to \tpb can be skipped.
For a tough lexeme \V{tuff}, the recognizer $(\V{tuff}, \V{off})$
might overapproximate, so that an unnecessary call
to \tpb might be made.

In practice,
allowed and token-consistent lexemes are sparse,
most lexemes are sliceable,
and most tough lexemes have good overapproximations.
So even at Stage 1,
this optimization should be significant.
But a more substantial payoff should be realized in Stage 2.

\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
\begin{tabular}{p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}p{30pt}}
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
        \multicolumn{2}{c}{ALA} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} \\
	% ===
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} &
        \multicolumn{2}{c|}{TB} &
        \multicolumn{2}{c}{Current} \\
	% ===
	\rule{30pt}{0pt} &
        \multicolumn{2}{c|}{} &
	\multicolumn{1}{c|}{\rule{30pt}{0pt}} &
	\multicolumn{1}{c|}{\rule{30pt}{0pt}} &
	\rule{30pt}{0pt} &
	\rule{30pt}{0pt} \\
	% ===
\cline{1-5}\cdashline{6-7}
\multicolumn{4}{|c|}{accepted lexeme} &
	\multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working lexeme} \\
\cline{1-5}\cdashline{6-7}
        \rule{30pt}{0pt} &
        \multicolumn{2}{|p{6em}|}{\centering committed token} &
        \multicolumn{3}{|p{114pt};{2pt/2pt}}{\centering\vspace{-1pt}working token}
        \\
	% ===
\cline{1-5}\cdashline{6-7}
\multicolumn{5}{|c|}{seen input} &
        \multicolumn{2}{|c}{unseen input} \\
\cline{1-5}\cdashline{6-7}
\end{tabular}
\vspace{6pt}
\caption{Input structure with token split between lexemes}
\label{fig:input-3}
\end{figure}

\FloatBarrier

\section{Caching}
\label{sec:caching}

Stage 1 relies on,
for every node \V{node},
every lexeme \V{lex},
and every offset \V{off}
accessing the value of
\begin{equation}
\label{eq:ruby-triple-test}
	\VFA{toString}{node} \, \sim \, (\V{lex}, \V{off}).
\end{equation}
We call
\begin{equation}
\label{eq:ruby-triple}
	(\V{node}, \V{lex}, \V{off})
\end{equation}
a ``Ruby triple'',
To really achieve optimization,
The  results of \eqref{eq:ruby-triple-test}
for \eqref{eq:ruby-triple}
should be cached.
This is possible when \V{lex} is sliceable,
because then the value of \eqref{eq:ruby-triple-test}
depends only on the values of
\V{node}, \V{lex}, and \V{off}
so that
\begin{equation}
\label{eq:ruby-function}
	\FA{Ruby}{\V{node}, \V{lex}, \V{off}}
\end{equation}
is a boolean function.
We can make \eqref{eq:ruby-function}
a total function by using the wildcard slice
when \V{lex} is tough.

\section{Examples}
\label{sec:examples}

\todo{Talk about negative offset examples}

\needspace{3in}
\FloatBarrier

\subsection {Example of fixed string lexeme}
{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{"abcdef"}} \\
		\hline
		LI Slice & & \texttt{/def/} \\
		\hline
		Token segments & \texttt{"abc"} & \texttt{"def"} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Fixed string example}
\label{fig:example-fixed}
\end{figure}
} % group for floatstyle

Figure~\ref{fig:example-fixed} shows the situation if
we have committed a token ending in \texttt{"abc"},
and \VFA{toString}{node},
the string implied by \V{node} of the token trie,
is \texttt{"def"}.
If, for purposes of caching a result,
we are considering
the token-consistent lexeme \texttt{/abcdef/}
the relevant range is \texttt{[3,6]},
so that there is an LI slice: \texttt{/def/}.
The lexeme offset is
\[
	\V{off} = (\V{TB} \subtract \V{ALA}) = 3,
\]
which is positive.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
\
\begin{equation}
\begin{gathered}
(\V{lex}, \V{off}) \\
	= \Velement{lex}{\V{off}, (\Vsize{node}+\V{off}) } \\
	= \Velement{(\texttt{/abcdef/})}{3, (3+3) } \\
    = \texttt{/def/}.
\end{gathered}
\end{equation}
Again, \VFA{toString}{node},
the string implied by \V{node} of the token trie,
is \texttt{"def"} and
\[
     \texttt{"def"} \sim \texttt{/def/},
\]
which is a match,
so we would cache a boolean true
for the Ruby triple
\[
	(\V{node}, \V{lex}, \V{off}).
\]

\needspace{3in}
\FloatBarrier
\subsection {Example for Kleene star regex}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/aa*/}} \\
		\hline
		LI Slice & & \texttt{/aaa/} \\
		\hline
		Token segments & \texttt{"aaa"} & \texttt{"aaa"} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Kleene regex star example}
\label{fig:example-kleene}
\end{figure}
} % group for floatstyle

Figure~\ref{fig:example-kleene} illustrates the handling of Kleene star
regular expressions.
In Figure~\ref{fig:example-kleene}
we have committed a token ending in \texttt{"aaa"},
and \VFA{toString}{node}
is also \texttt{"aaa"}.
If, for purposes of caching a result,
we are considering
the token-consistent lexeme \texttt{/aa*/}
the relevant range is \texttt{[3,6]},
there is an LI slice: \texttt{/aaa/}.\footnote{%
	For Kleene star regular expressions there will not necessarily
	be an LI slice.
	In that case we use overapproximation.
	See Section~\ref{sec:overapproximation-example}
	on page~\pageref{sec:overapproximation-example}
	for an
	example of overapproximation, in that case for regular
	expressions using alternatives.
}
The lexeme offset is
\[
	\V{off} = (\V{TB} \subtract \V{ALA}) = 3,
\]
which is positive.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
\
\begin{equation}
\begin{gathered}
(\V{lex}, \V{off}) \\
	= \Velement{lex}{\V{off}, (\Vsize{node}+\V{off}) } \\
	= \Velement{(\texttt{/aa*/})}{3, (3+3) } \\
    = \texttt{/aaa/}.
\end{gathered}
\end{equation}
Again, \VFA{toString}{node}
is \texttt{"aaa"}.
\[
     \texttt{"aaa"} \sim \texttt{/aaa/}
\]
is a match,
so we would cache a boolean true
for the Ruby triple
$(\V{node}, \V{lex}, \V{off})$.

\needspace{3in}
\FloatBarrier
\subsection {Example without LI slice}
\label{sec:example-no-li-slice}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[H]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/(aaaaaa|bbbbbb)/}} \\
		\hline
		LI Slice	& & none \\
		\hline
		Token segments & \texttt{"aaa"} & \texttt{"bb"} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{No LI slice example}
\label{fig:example-no-li-slice}
\end{figure}
} % group for floatstyle

In the case of
Figure~\ref{fig:example-no-li-slice},
we are considering the lexeme
\texttt{/(aaaaaa|bbbbbb)/} for which
there is no LI slice.
We must remedy this in order
to determine a boolean true
for the Ruby triple
$(\V{node}, \V{lex}, \V{off})$.
Figure~\ref{fig:example-factored}
on page~\pageref{fig:example-factored}
shows a solution that involves factoring
\texttt{/(aaaaaa|bbbbbb)/}
into two lexemes,
one for each alternative.
Figure~\ref{fig:example-overapproximation}
on page~\pageref{fig:example-overapproximation}
shows a solution that uses
overapproximation.

\needspace{3in}
\FloatBarrier
\subsection{Example of factored alternatives}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[ht]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		\multirow{2}{*}{Lexemes}
			& \multicolumn{2}{|c|}{\texttt{/aaaaaa/}} \\
			 & \multicolumn{2}{|c|}{\texttt{/bbbbbb/}} \\
		\hline
		\multirow{2}{*}{LI Slices} & & \texttt{/aaa/} \\
		 & & \texttt{/bbb/} \\
		\hline
		Token segments & \texttt{"aaa"} & \texttt{"aaa"} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Factored regular expression example}
\label{fig:example-factored}
\end{figure}
} % group for floatstyle

One solution to the problem presented in
Section~\ref{sec:example-no-li-slice}
on page~\pageref{sec:example-no-li-slice}
is ``factoring''.
Any regular expression which does not use the Kleene star
can be factored into fixed string lexemes.
We do note that such regular expressions might still be tough:
the number of factored strings can be large enough to make
a factoring solution impractical.

In this example, factoring is quite practical.
\texttt{/(aaaaaa|bbbbbb)/}
can be factored into the two lexemes
\texttt{/aaaaaa/} and \texttt{/bbbbbb/}.
As in the previous examples, the offset is 3,
and the range is \texttt{[3,6]}.
Applying the formula for slices of positive offsets~\eqref{eq:positive-offset},
their slices are easily found to be, respectively,
\texttt{/aaa/} and \texttt{/bbb/}.
\VFA{toString}{node} is \texttt{"aaa"},
so that
\texttt{/aaa/} matches
and \texttt{/bbb/} does not.
We cache a boolean true
for the Ruby triple
$(\V{node}, \texttt{/aaaaaa/}, 3)$
and a boolean false
for the Ruby triple
$(\V{node}, \texttt{/bbbbbb/}, 3)$.

\subsection {Example of overapproximation}
\label{sec:overapproximation-example}

{%
\floatstyle{plain}
\restylefloat{figure}
\noindent
\begin{figure}[H]
\vspace{6pt}
% note: default tabcolsep is 6pt
% \rule{30pt}{0pt} needed to trigger use of minimum p cell width
	\begin{tabular}{|c|r|l|}
		\hline
		Range of locations & \multicolumn{1}{|c|}{[ALA,TB]}
			& \multicolumn{1}{|c|}{[TB,Current]}
			\\
		\hline
		Lexeme	& \multicolumn{2}{|c|}{\texttt{/(aaaaaa|bbbbbb)/}} \\
		\hline
		Overapproximated LI Slice & & \texttt{/[ab][ab]/} \\
		\hline
		Token segments & \texttt{"aaa"} & \texttt{"ab"} \\
		\hline
\end{tabular}
\vspace{6pt}
\caption{Overapproximated slice example}
\label{fig:example-overapproximation}
\end{figure}
} % group for floatstyle

\FloatBarrier

\section{Caching strategy}
\label{sec:caching-strategy}


It may be best to vary the caching strategy.\footnote{%
	In this section we discuss varying the caching strategy
	only for different absolute values of \V{off}.
	Further investigation
	may show that it makes sense to
	take into account the sign of \V{off},
	or to vary the caching strategy by
	lexeme.
}
For small absolute values of \V{off}, the cache
will be dense,
and the likelihood of hits in a fully pre-populated
cache will be high.
In this case,
pre-computation at grammar compile time makes sense.
Computing many of these values in a single token trie
traversal would allow for optimizations.

\begin{MYsloppy}
Lexemes may be of arbitrary length,
so that, in practice,
pre-computation of all Ruby triple values
is not just costly, but impossible.
Further,
in practical applications,
the lexemes of
very large absolute values of \V{off}
will usually be very long strings or here-documents.
Long strings and here-documents
are unlikely to be repeated exactly,
so that the likelihood of cache hits would
be very low.
Therefore, for absolute values of \V{off}
above a certain threshold,
it is likely to make sense to not cache the
value of Ruby triples.
\end{MYsloppy}

\section{Stage 2: Track ends of lexeme}
\label{sec:stage2}

We call a location where a lexeme can end,
an ``end of lexeme'', or EOL.
In Stage 2, we build on Stage 1 by tracking EOLs.
In cases where \tpbb
is called when we are not at an EOL,
\tpbb simply stores byte \V{b}.
If we skip the call to \tpbb,
but turn out to need the value of byte \V{b}
later, we can find it from context.
Note that,
while Stage 1 required no changes in the logic of
\tpb,
Stage 2 may require some modifications.

To implement stage 2,
we will want to cache a boolean that indicates
whether every Ruby triple is at an end of lexeme.
\begin{equation}
\label{eq:eol-function}
	\FA{EOL}{\V{node}, \V{lex}, \V{off}}
\end{equation}
We need not cache values of
\eqref{eq:eol-function} when
the \Fname{Ruby} function~\eqref{eq:ruby-function}
is false.
\eqref{eq:eol-function} should be true,
whenever \V{lex} is an unsliceable lexeme.

In stage 2, calls to \tpbb
need only be made at end of lexeme,
and for tough lexemes.
Since for most nodes there none of the
tough lexemes will be token-consistent,
and most toktrie nodes will not match
most Ruby triples,
calls to \tpbb
should be reduced greatly.

\section{Forcing}

The optimization described here detects all situations where there is
only one token-consistent lexeme at a location.
In this way, it can be said to auto-detect all cases of forcing.
This is done as a natural extension of its usual logic for
token-consistent lexemes and their tokenization,
rather than by special-casing.

\FloatBarrier

{
\clearpage

% Ragged right, do not hyphenate.
\RaggedRight
\hyphenpenalty=10000
\exhyphenpenalty=10000

% Silence current hbox warnings, but allow them if
% badness increases further
\hbadness=2000

\begin{thebibliography}{10}

\bibitem{Kegler2011a}
\newblock{Jeffrey Kegler.}
\newblock{``What is the Marpa algorithm?''.}
\newblock{\url{https://blogs.perl.org/users/jeffrey_kegler/2011/11/what-is-the-marpa-algorithm.html}.
   Retrieved 2 December, 2024.}

\bibitem{Kegler2011b}
\newblock{Jeffrey Kegler.}
\newblock{``Marpa and the Ruby Slippers''.}
\newblock{\url{http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2011/11/marpa-and-the-ruby-slippers.html}.
   Retrieved 2 December, 2024.}

\bibitem{Kegler2023}
\newblock{Jeffrey Kegler.}
\newblock{``Marpa, A practical general parser: the recognizer''.}
\newblock{\url{https://arxiv.org/abs/1910.08129}.
   Retrieved 2 December, 2024.}

\bibitem{llguidance}
\newblock{llguidance team.}
\newblock{"Low-level guidance parser".}
\newblock{
  \url{https://github.com/microsoft/llguidance}.
  Retrieved 2 December, 2024.}

\bibitem{wiki-loop-invariant}
\newblock{Wikipedia contributors.}
\newblock{"Loop-invariant code motion --- {Wikipedia}{,} The Free Encyclopedia".}
\newblock{\url{https://en.wikipedia.org/w/index.php?title=Loop-invariant_code_motion&oldid=1249559170}.
	Retrieved 16 December 2024.}

\bibitem{XGrammar2024}
\newblock{Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao and Tianqi Chen.}
\newblock{``XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models''.}
\newblock{\url{https://arxiv.org/abs/2411.15100}.
	Retrieved 16 December 2024.}

\end{thebibliography}

} % RaggedRight

% \clearpage
% \phantomsection
% \listofalgorithms

\ifdraft{
    \clearpage
    \phantomsection
    \listoftodos
}{}

\end{document}
